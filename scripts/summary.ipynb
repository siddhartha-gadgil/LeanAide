{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79debff1-a7e9-4357-90e3-a0d509b9d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e04e81-bf23-456d-b080-6d5e644bb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldman='''\n",
    "\n",
    "\\section{Introduction}\n",
    "\n",
    "Given a homotopy equivalence between manifolds of the same dimension, a fundamental question in topology is whether it is homotopic to a homeomorphism. Basic examples of exotic homotopy equivalences -- ones that are not  homotopic to the homeomorphisms, are maps between surfaces with boundary. For example, the three-holed sphere and the one-holed torus are homotopy equivalent but not homeomorphic.\n",
    "\n",
    "We assume throughout that all surfaces we consider are oriented. In this note, we show that there is a simple and natural characterisation of when a homotopy equivalence $f\\co \\Sigma_1\\to\\Sigma_2$ between compact surfaces is homotopic to a homeomorphism in terms of the \\emph{Goldman bracket}, which is a Lie Algebra structure associated to a surface. More precisely, if $\\hpi(\\Sigma)$ denotes the set of free homotopy classes of closed curves in $\\Sigma$, then the Goldman bracket (whose definition we recall below) is a bilinear map\n",
    "$$[\\cdot,\\cdot]\\co \\Z[\\hpi(\\Sigma)]\\times\\Z[\\hpi(\\Sigma)]\\to\\Z[\\hpi(\\Sigma)],$$\n",
    "which is skew-symmetric and satisfies the Jacobi identity.\n",
    "\n",
    "\\begin{theorem}\\label{main}\n",
    "A homotopy equivalence $f\\co \\Sigma_1\\to \\Sigma_2$ between compact, connected, oriented surfaces with non-empty boundary is homotopic to a homeomorphism if and only if it commutes with the Goldman bracket, i.e., for all $x,y\\in\\Z[\\hpi(\\Sigma_1)]$, we have,\n",
    "\\begin{equation}\\label{preserve}\n",
    "[f_*(x),f_*(y)]=f_*([x,y]),\n",
    "\\end{equation}\n",
    "where $f_*\\co \\Z[\\hpi(\\Sigma_1)]\\to\\Z[\\hpi(\\Sigma_1)]$ is the function induced by $f$.\n",
    "\\end{theorem}\n",
    "\n",
    "For closed surfaces, every homotopy equivalence is homotopic to a  homeomorphism. So the corresponding result holds trivially in this case.\n",
    "\n",
    "Homotopy equivalences that are homotopic to homeomorphisms can be characterised as those that preserve the so called \\emph{peripheral structure}. However, our result has the advantage that the characterization is in terms of a structure defined without reference to the boundary. Furthermore, the Goldman bracket is the simplest instance of so called \\emph{string topology}~\\cite{CS1}, which in turn is related to the Floer homology of the cotangent bundle~\\cite{AS}.\n",
    "\n",
    "\\section{Preliminaries}\n",
    "\n",
    "\\subsection{The Goldman bracket}\n",
    "We recall the definition of the Goldman bracket~\\cite{Gol} on a connected surface $\\Sigma$. Let $\\hpi(\\Sigma)$ denote the set of free homotopy classes of curves in $\\Sigma$. For a closed curve $\\alpha$, let $\\langle \\alpha\\rangle\\in\\hpi(\\Sigma)$ denote its homotopy class.\n",
    "\n",
    "Given homotopy classes $x$ and $y$ of closed curves, we consider smooth, oriented representatives $\\alpha$ and $\\beta$ that intersect transversally in double points.  The bracket of $x$ and $y$ is then defined as the sum\n",
    "\\begin{equation}\\label{bracket}\n",
    "[x,y]=\\sum_{p\\in \\alpha\\cap\\beta} \\varepsilon_p\\ \\langle\\alpha*_p\\beta\\rangle,\n",
    "\\end{equation}\n",
    "where, for $p\\in\\alpha\\cap\\beta$, $\\varepsilon_p$  is the sign of the intersection between $\\alpha$ and $\\beta$ at $p$  and $\\alpha*_p\\beta$ is the product of the loops $\\alpha$ and $\\beta$ viewed as based at $p$.\n",
    "\n",
    "This expression is well defined by the first part of the following remarkable theorem of Goldman. A completely topological proof of this has been given by Chas~\\cite{chas}.\n",
    "\n",
    "\\begin{theorem}[Goldman~\\cite{Gol}]\\label{gold}\n",
    "The bracket defined by Equation~\\ref{bracket} has the following properties.\n",
    "\\begin{enumerate}\n",
    "\\item Equation~\\ref{bracket} gives a well-defined bilinear map $\\Z[\\hpi(\\Sigma)]\\times\\Z[\\hpi(\\Sigma)]\\to\\Z[\\hpi(\\Sigma)]$, i.e., the right hand side of the equation depends only on the homotopy classes of $\\alpha$ and $\\beta$.\n",
    "\\item The bracket is skew-symmetric and satisfies the Jacobi identity.\n",
    "\\item If $\\alpha$ is a simple curve and $x=\\langle\\alpha\\rangle$, then, for  $y\\in\\hpi(\\Sigma)$, $[x,y]=0$ if and only if $y=\\langle\\beta\\rangle$ for a closed curve $\\beta$ such that $\\beta$ is disjoint from $\\alpha$.\n",
    "\\end{enumerate}\n",
    "\\end{theorem}\n",
    "\n",
    "\\subsection{Peripheral classes}\n",
    "\n",
    "For a connected surface $\\Sigma$, if $p\\in\\Sigma$ is a point, $\\hpi(\\Sigma)$ is the set of conjugacy classes in $\\pi_1(\\Sigma,p)$. The power operations $\\pi_1(\\Sigma,p)\\to\\pi_1(\\Sigma,p)$, $\\alpha\\mapsto\\alpha^n$, $n\\in\\Z$, and the inverse function $\\alpha\\mapsto\\alpha^{-1}$ on $\\pi_1(\\Sigma,p)$ induce well-defined functions on $\\hpi(\\Sigma)$. A class $x\\in\\hpi$ that is not a non-trivial power is called \\emph{primitive}.\n",
    "\n",
    "Suppose henceforth that $\\Sigma$ is a compact, connected surface with non-empty boundary. An element of $\\hpi$ is said to be \\emph{peripheral} if it can be represented by a curve $\\alpha\\subset \\del \\Sigma$.\n",
    "\n",
    "Assume further that $\\Sigma$ has negative Euler characteristic, i.e., $\\Sigma$ is not a disc or annulus. Then each component of $\\del \\Sigma$ corresponds to a pair of primitive peripheral classes (one for each orientation of the boundary curve) which are inverses of each other. Further, the primitive peripheral classes corresponding to different boundary components are different.\n",
    "\n",
    "\\section{Proof of Theorem~\\ref{main}}\n",
    "\n",
    "We now turn to the proof of Theorem~\\ref{main}. It is clear from the definition that a homeomorphism commutes with the Goldman bracket. As homotopic maps induce the same function on $\\hpi$, it follows that if a homotopy equivalence $f\\co \\Sigma_1\\to\\Sigma_2$ is homotopic to a homeomorphism, then $f_*$ commutes with the Goldman bracket.\n",
    "\n",
    "The rest of the paper is devoted to proving the converse. Assume henceforth that $f\\co \\Sigma_1\\to\\Sigma_2$ is a map between connected, compact, oriented surfaces with non-empty boundary so that for all $x,y\\in\\Z[\\hpi(\\Sigma_1)]$, we have,\n",
    "\\[\n",
    "[f_*(x),f_*(y)]=f_*([x,y]).\n",
    "\\]\n",
    "\n",
    "\\begin{lemma}\\label{perinul}\n",
    "For a compact surface $\\Sigma$ with boundary, a non-trivial class $x\\in\\hpi(\\Sigma)$ is peripheral if and only if for all $y\\in\\hpi(\\Sigma)$, we have\n",
    "$[x,y]=0.$\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "Assume $x$ is peripheral, i.e., $x=\\langle\\alpha\\rangle$ with $\\alpha\\subset \\del\\Sigma$. As any closed curve in $\\Sigma$ is freely homotopic to one in the interior of $\\Sigma$,  every element $y\\in \\hpi(\\Sigma)$ can be represented by a curve $\\beta\\subset int(\\Sigma)$. Hence $\\alpha\\cap\\beta=\\phi$, which implies by Equation~\\ref{bracket} that $[x,y]=0$.\n",
    "\n",
    "Conversely, let $x=\\langle\\alpha\\rangle$ be such that $[x,y]=0$ for all $y\\in\\hpi(\\Sigma)$. If $x$ is not peripheral, it is well known that there is a simple closed curve $\\beta$ so that the geometric intersection number of $\\alpha$ and $\\beta$ is non-zero, i.e., $\\alpha$ is not homotopic to a curve that is disjoint from $\\beta$. This follows from the fact that there is a pair of simple curves in $\\Sigma$ that \\emph{fill} $\\Sigma$ (i.e., all curves with zero geometric intersection with each of them is peripheral), or alternatively by considering the geodesic representative for $x$ with respect to a hyperbolic metric and using the result that geodesics intersect minimally.\n",
    "\n",
    "Now, by Goldman's theorem (Theorem~\\ref{gold}, part~(3)), as the simple closed curve $\\beta$ has non-zero geometric intersection number with $\\alpha$ and $y=\\langle \\beta\\rangle$, we have $[x,y]\\neq 0$, a contradiction.\n",
    "\n",
    "\\end{proof}\n",
    "\n",
    "Recall that we assume that we have a map $f\\co \\Sigma_1\\to\\Sigma_2$ such that $f_*$ commutes with the Goldman bracket. In case $\\Sigma_1$ (hence $\\Sigma_2$) is a disc or an annulus, it is easy to see that any homotopy equivalence is homotopic to a homeomorphism. We can hence assume henceforth that $\\Sigma_1$ and $\\Sigma_2$ have negative Euler characteristic.\n",
    "\n",
    "\\begin{lemma}\\label{imgbdy}\n",
    "Suppose $f\\co \\Sigma_1\\to\\Sigma_2$ induces a surjection $f_*\\co \\hpi(\\Sigma_1)\\to\\hpi(\\Sigma_2)$. Then if $x\\in\\hpi(\\Sigma_1)$ is a peripheral class, so is $f_*(x)$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "As $x$ is peripheral, by Lemma~\\ref{perinul} $[x,y]=0$ for all $y\\in\\hpi(\\Sigma_1)$. As $f_*$ commutes with the Goldman bracket, it follows that $[f_*(x),f_*(y)]=0$ for all $y\\in\\hpi(\\Sigma_1)$. As $f_*$ is surjective, $[f_*(x),z]=0$ for all $z\\in\\hpi(\\Sigma_2)$. It follows by Lemma~\\ref{perinul} that $f_*(x)$ is peripheral.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{lemma}\\label{bdyinv}\n",
    "Suppose $f\\co \\Sigma_1\\to\\Sigma_2$ induces an injection $f_*\\co \\hpi(\\Sigma_1)\\to\\hpi(\\Sigma_2)$. Then, for $x\\in\\hpi(\\Sigma_1)$, if $f_*(x)$ is peripheral then $x$ is peripheral.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "As $f_*(x)$ is peripheral, by Lemma~\\ref{perinul} $[f_*(x),f_*(y)]=0$ for all $y\\in\\hpi(\\Sigma_1)$. As $f_*$ is injective and commutes with the Goldman bracket, it follows that $[x,y]=0$ for all $y\\in\\hpi(\\Sigma_1)$. Thus, $x$ is peripheral by Lemma~\\ref{perinul}.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{lemma}\\label{bdyhomotbdy}\n",
    "Suppose $f\\co \\Sigma_1\\to\\Sigma_2$ is a homotopy equivalence and $C_1$, $C_2$, \\dots $C_n$ are the boundary components of $\\Sigma_1$. Then there are boundary components $C'_1$, $C_2'$,\\dots, $C'_n$ of $\\Sigma_2$ such that $f(C_i)$ is homotopic to $C_i'$ for all $i$, $1\\leq i\\leq n$. Further the boundary components $C_i'$ are all distinct and $\\del\\Sigma_2=C_1'\\cup C_2'\\cup\\dots\\cup C_n'$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "As $f$ is a homotopy equivalence, $f_*\\co \\hpi(\\Sigma_1)\\to\\hpi(\\Sigma_2)$ is both surjective and injective and maps primitive classes to primitive classes. For $1\\leq i\\leq n$, let $\\alpha_i$ be a closed curve parametrizing $C_i$. By Lemma~\\ref{imgbdy}, $\\alpha_i'=f(\\alpha_i)$ represents a primitive peripheral class for $1\\leq i\\leq n$, and hence represents a boundary component $C'_i$. Furthermore, as $\\Sigma_1$ has negative Euler characteristic, for $i\\neq j$, $\\alpha_i\\neq \\alpha_j^{\\pm 1}$, and hence $\\alpha'_i\\neq {\\alpha'}_j^{\\pm 1}$. It follows that the curves $\\alpha_i'$ are homotopic to curves representing distinct boundary components $C_i'$, $1\\leq i\\leq n$.\n",
    "\n",
    "Finally, we see that $\\del\\Sigma_2=C_1'\\cup C_2'\\cup\\dots\\cup C_n'$. Namely, if $\\gamma$ is a curve parametrizing a boundary component $C''$ of $\\del\\Sigma_2$, $\\langle\\gamma\\rangle=f_*(x)$ for some class $x$ (as $f_*$ is surjective). The class $x$ must be peripheral  by Lemma~\\ref{bdyinv} and primitive as $\\gamma$ is primitive. Hence $x$ is represented by either $\\alpha_i$ or $\\alpha_i^{-1}$ for some $i$, $1\\leq i\\leq n$. In either case, $C''=C_i'$.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{lemma}\\label{bdy2bdy}\n",
    "Suppose $f\\co \\Sigma_1\\to\\Sigma_2$ is a homotopy equivalence. Then $f$ is homotopic to a map $g\\co \\Sigma_1\\to\\Sigma_2$ so that $g(\\del \\Sigma_1)=\\del\\Sigma_2$ and $g|_{\\del\\Sigma_1}\\co  \\del\\Sigma_1\\to\\del\\Sigma_2$ is a homeomorphism.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "By Lemma~\\ref{bdyhomotbdy}, the restriction of $f$ to $\\del\\Sigma_1$ is homotopic in $\\Sigma_2$ to a homeomorphism $\\varphi\\co \\del\\Sigma_1\\to\\del\\Sigma_2$. Let $h\\co \\del\\Sigma_1\\times [0,1]\\to\\del\\Sigma_2$ be such a homotopy, i.e., $h$ is a map so that $h(\\cdot,0)=f(\\cdot)$ and $h(\\cdot,1)=\\varphi(\\cdot)$.\n",
    "\n",
    "Let $\\Sigma_1'=\\Sigma_1\\coprod_{\\del\\Sigma_1=(\\del\\Sigma_1\\times\\{0\\})} \\del\\Sigma_1\\times [0,1]$ be the space obtained from the disjoint union of $\\Sigma_1$ and $\\del\\Sigma_1\\times [0,1]$ by identifying points $x\\in\\del\\Sigma_1\\subset \\Sigma_1$ with the corresponding points $(x,0)\\in \\del\\Sigma_1\\times [0,1]$. Define a map $g'\\co \\Sigma_1'\\to\\Sigma_2$ by $g'(x)=f(x)$ for $x\\in \\Sigma_1$ and $g'(x,t)=h(x,t)$ for $(x,t)\\in \\del\\Sigma_1\\times [0,1]$.\n",
    "\n",
    "Observe that $\\del\\Sigma_1'=\\del\\Sigma_1\\times\\{1\\}$. The inclusion map $\\Sigma_1\\to\\Sigma_1'$ is homotopic to a homeomorphism $\\psi$ so that, for $x\\in\\del\\Sigma_1$, $\\psi(x)=(x,1)$. Let $H'\\co \\Sigma_1\\to\\Sigma_1'$ be such a homotopy, i.e., $H'$ is a map so that $H'(\\cdot,0)$ is the inclusion map and $H'(\\cdot, 1)=\\psi(\\cdot)$. Let $g\\co \\Sigma_1\\to\\Sigma_2$ be given by\n",
    "$$g(x)=g'(H'(x,1))=g'(\\psi(x)).$$\n",
    "Then, for $x\\in\\del\\Sigma_1$, $g(x)=g'(\\psi(x))=g'((x,1))=h(x,1)=\\varphi(x)$, i.e. $g\\vert_{\\del \\Sigma_1}=\\varphi$. Hence $g$ is a map from $\\Sigma_1\\to\\Sigma_2$ so that $g(\\del \\Sigma_1)=\\del\\Sigma_2$ and $g|_{\\del\\Sigma_1}\\co  \\del\\Sigma_1\\to\\del\\Sigma_2$ is a homeomorphism. Finally, for $x\\in\\Sigma_1$, $g'(H'(x,0))=g'(x)=f(x)$. Hence,\n",
    "a homotopy from $f$ to $g$ is given by\n",
    "$$H(x,t)=g'(H(x,t)).$$\n",
    "\\end{proof}\n",
    "\n",
    "By a theorem of Nielsen, $g$, and hence $f$, is homotopic to a homeomorphism  (see Lemma 1.4.3 of~\\cite{Wa} -- as $g\\colon(\\Sigma_1,\\del\\Sigma_1)\\to (\\Sigma_2,\\del\\Sigma_2)$ induces an isomorphism of fundamental groups, it follows that $g$ is homotopic to a homeomorphism). This completes the proof of Theorem~\\ref{main}.\\qed\n",
    "\n",
    "\n",
    "\\end{thebibliography}\n",
    "\n",
    "\\end{document}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34599bf-720c-4775-95c1-36ef60fa7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = summarise(goldman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fbf44b-62bb-4fd0-a80b-d2aab239ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Homotopy Equivalence and Goldman Bracket\n",
      "\n",
      "The paper discusses the relationship between homotopy equivalence and homeomorphisms of manifolds of the same dimension, and offers a characterization through the concept of the Goldman bracket, a Lie Algebra structure linked to a surface.\n",
      "\n",
      "Definition:\n",
      "The Goldman bracket is defined as a bilinear map symbolized by [\\cdot,\\cdot], which takes input from the set of free homotopy classes of closed curves, denoted by \\hpi(\\Sigma), and maps to the same set. The map is skew-symmetric and satisfies the Jacobi identity.\n",
      "\n",
      "Main Theorem:\n",
      "A homotopy equivalence f between compact, oriented surfaces with non-empty boundaries is homotopic to a homeomorphism if and only if it commutes with the Goldman bracket. That is to say, for all x,y in \\Z[\\hpi(\\Sigma_1)], we have [f_*(x),f_*(y)]=f_*([x,y]), where f_* is the function induced by f.\n",
      "\n",
      "Proof:\n",
      "The paper provides a number of lemmas to support the proof of the main theorem. The lemmas cover definitions and properties of peripheral classes, characteristics of Goldman brackets, and the relationship between maps and homotopy equivalences. \n",
      "\n",
      "The proof of the theorem then follows by showing that any homotopy equivalence that commutes with the Goldman bracket can be homotopically transformed to a homeomorphism. This is achieved through a series of steps involving multiple lemmas demonstrating that peripheral classes correspond under the map, the map can be homotopically adjusted to align the boundaries, and finally applying the Nielsen's theorem, the homotopy equivalence can be homotopically deformed to a homeomorphism. \n",
      "\n",
      "The paper thus concludes that the Goldman bracket provides a way to determine if a homotopy equivalence is homotopic to a homeomorphism.\n",
      "---------------------------\n",
      "The mathematical text discusses the relationship between homotopy equivalences and homeomorphisms for manifolds of the same dimension. The key tools used in this study are the Goldman bracket, which provides a Lie algebra structure to a surface, and peripheral classes of curves on a surface.\n",
      "\n",
      "The main theorem (labeled \"main\" in the text) states that a homotopy equivalence $f$ between compact, connected, oriented surfaces with non-empty boundary is homotopic to a homeomorphism if and only if it commutes with the Goldman bracket. This means that for all $x,y$ in the set of free homotopy classes of closed curves in the first surface, $[f_*(x),f_*(y)]=f_*([x,y])$, where $f_*$ is the function induced by $f$.\n",
      "\n",
      "The Goldman bracket is defined for two homotopy classes of closed curves on a connected surface such that their representatives intersect transversally in double points. The bracket of these two classes is defined as the sum of the signed intersections of the representative curves. It is shown to be a well-defined bilinear map on the set of free homotopy classes of closed curves on the surface, and satisfies the Jacobi identity and skew-symmetry properties.\n",
      "\n",
      "A sequence of lemmas is used to prove the main theorem. They establish results on the behavior of the Goldman bracket and peripheral classes under maps between surfaces, homotopy classes of boundary components, and the existence of a homotopy of a given map to one that takes the boundary of one surface to the boundary of the other.\n",
      "\n",
      "The final result says that a homotopy equivalence between surfaces that commutes with the Goldman bracket is homotopic to a homeomorphism. This ties homotopy equivalences that preserve a certain algebraic structure (the Goldman bracket) to geometric properties (being homotopic to a homeomorphism).\n",
      "---------------------------\n",
      "The text presents a theorem and its proof concerning the properties of a homotopy equivalence between compact surfaces. The main definitions introduced are the Goldman bracket, a Lie Algebra structure related to a surface, and a peripheral (and primitive) class, which are certain types of homotopy classes of curves on a surface. \n",
      "\n",
      "The main theorem states that a homotopy equivalence between compact, connected, oriented surfaces with non-empty boundary is homotopic to a homeomorphism if and only if it commutes with the Goldman bracket. In other words, if for all x,y in Z[π1(Σ1)] we have [f*(x),f*(y)]=f*([x,y]), where f* is the function induced by the homotopy equivalence. \n",
      "\n",
      "The proof of the theorem is structured around several lemmas, which establish different properties of the Goldman bracket and its relation to peripheral classes, and also demonstrate how these properties are preserved under homotopies. The proof concludes with a reference to Nielsen's theorem to show that under the given conditions, a homotopy equivalence is indeed homotopic to a homeomorphism. \n",
      "\n",
      "The text also mentions that Goldman bracket is related to the field of string topology and Floer homology of the cotangent bundle, indicating a broader mathematical context for the results presented.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "print_all(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500ce778-dfcf-4d0a-b61b-12b2d656cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Goldman bracket is a Lie algebra structure that can be associated with a surface, and is used to study the geometry and topology of the surface. It is defined on the free homotopy classes of closed curves in a surface.\n",
      "\n",
      "More precisely, let $\\Sigma$ be a connected surface and $\\hpi(\\Sigma)$ denote the set of free homotopy classes of closed curves in $\\Sigma$. The Goldman bracket is a bilinear map\n",
      "\n",
      "$$[\\cdot,\\cdot]: \\Z[\\hpi(\\Sigma)] \\times \\Z[\\hpi(\\Sigma)] \\to \\Z[\\hpi(\\Sigma)],$$\n",
      "\n",
      "which is skew-symmetric and satisfies the Jacobi identity.\n",
      "\n",
      "Given homotopy classes $x$ and $y$ of closed curves, we consider smooth, oriented representatives $\\alpha$ and $\\beta$ that intersect transversally in double points. The bracket of $x$ and $y$ is then defined as the sum\n",
      "\n",
      "$$[x,y]=\\sum_{p\\in \\alpha\\cap\\beta} \\epsilon_p \\langle\\alpha*_p\\beta\\rangle,$$\n",
      "\n",
      "where, for $p\\in\\alpha\\cap\\beta$, $\\epsilon_p$ is the sign of the intersection between $\\alpha$ and $\\beta$ at $p$ and $\\alpha*_p\\beta$ is the product of the loops $\\alpha$ and $\\beta$ viewed as based at $p$.\n",
      "\n",
      "This definition is well-defined, i.e., the right hand side of the equation depends only on the homotopy classes of $\\alpha$ and $\\beta$; it does not depend on the choice of representatives. This is a nontrivial fact and is part of the so-called Goldman's theorem.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "gold_query=f'''Use the following text to answer the given questions:\n",
    "\n",
    "{goldman}\n",
    "\n",
    "---\n",
    "\n",
    "Question: Define the Goldman bracket, including the domain of definition\n",
    "'''\n",
    "\n",
    "defs = math(gold_query, deployment_name='leanaide-gpt4-32', n=1)\n",
    "\n",
    "print_all(defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9b93db-f693-48c2-980e-b289e909f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theorem of Nielsen used in the above work is a result about the existence of homeomorphisms between topological spaces under certain conditions. Here is the precise statement:\n",
      "\n",
      "Nielsen's Theorem: Let $X$ and $Y$ be compact, connected, oriented surfaces (possibly with boundary). Suppose $f: X \\to Y$ is a continuous map that induces an isomorphism on fundamental groups. Then $f$ is homotopic to a homeomorphism.\n",
      "\n",
      "In the context of the above work, this theorem is used to prove that a homotopy equivalence between surfaces that commutes with the Goldman bracket (and thus preserves a certain algebraic structure) is homotopic to a homeomorphism. Specifically, it's used in the very last step of the proof of the main theorem to show that a certain map $g: \\Sigma_1 \\to \\Sigma_2$ is homotopic to a homeomorphism. The authors cite this result as Lemma 1.4.3 from a reference work, presumably where Nielsen's theorem is stated and proved.\n",
      "---------------------------\n",
      "The theorem of Nielsen used in the work is as follows:\n",
      "\n",
      "Given a homotopy equivalence $g: (\\Sigma_1, \\del \\Sigma_1) \\to (\\Sigma_2, \\del \\Sigma_2)$ between two compact, connected surfaces with boundary that induces an isomorphism of fundamental groups and takes one boundary to the other, the map $g$ is homotopic to a homeomorphism.\n",
      "\n",
      "This theorem is referred to as Nielsen's realization problem and can be found in Lemma 1.4.3 of the cited work ~\\cite{Wa}. The key aspect of this theorem for the work is that it allows the transformation of a homotopy equivalence into a homeomorphism under certain conditions, which is crucial to the proof of Theorem~\\ref{main}.\n",
      "---------------------------\n",
      "The theorem of Nielsen used in the above work is the following:\n",
      "\n",
      "Nielsen's Theorem: If a continuous map between two compact, connected, oriented surfaces with non-empty boundary induces an isomorphism on the fundamental groups, then it is homotopic to a homeomorphism.\n",
      "\n",
      "In other words, if there exists a continuous function between these surfaces that preserves the fundamental group structure, it can be slightly adjusted (via a homotopy) to become a homeomorphism, which is a bicontinuous, bijective map that also preserves the topological structure of the surfaces. \n",
      "\n",
      "This theorem is used in the final step of the proof to argue that a particular homotopy equivalence is homotopic to a homeomorphism, finalizing the proof of the main theorem.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "nielsen_query=f'''Use the following text to answer the given questions:\n",
    "\n",
    "{goldman}\n",
    "\n",
    "---\n",
    "\n",
    "Question: State precisely the theorem of Nielsen used in the above work.\n",
    "'''\n",
    "\n",
    "nielsen = math(nielsen_query, deployment_name='leanaide-gpt4-32', n=3)\n",
    "\n",
    "print_all(nielsen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da12ffe-45ca-4eee-b3e6-3fa811876f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cross = r'''\\section{Introduction}\n",
    "\n",
    "Consider a word $X$ in an alphabet consisting of $4$ letters, with the letters viewed either as $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$ (i.e., generators of the free group  $\\F$ and their inverses, where we use the notation $\\bar{g}$ for $g^{-1}$) or $A$, $U$, $G$ and $C$ (i.e., nucleotides in an RNA sequence).  There is a natural notion of a length $\\ell(X)$ associated to such a word, which can be defined in several equivalent ways (see~\\cite{Ga1} and~\\cite{Ga2} for more details). We give three descriptions of $\\ell$, two of which (as we indicate below) generalize to random words in $2k$ letters, for $k\\geq 2$.\n",
    "\n",
    "\\begin{enumerate}\n",
    "  \\item If $X$ is viewed as a word in $\\F$ then $\\ell$ is the maximal conjugacy-invariant length function on $\\F$ which satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$. Equivalently, $\\ell$ is the word length in the generating set given by all conjugates $g\\alpha g^{-1}$, $g\\bar{\\alpha} g^{-1}$, $g\\bar{\\beta} g^{-1}$ and $g\\beta g^{-1}$ of the generators of $\\F$ and their inverses (where $\\bar{\\alpha}=\\alpha^{-1}$ and $\\bar{\\beta} = \\beta^{-1}$). More generally, an arbitrary word in $2k$ letters gives an element of $\\FF$, and $\\ell$ can be defined as a maximal conjugacy-invariant length function (or word length in conjugates of generators and their inverses) in this case too.\n",
    "  \\item If $X$ is viewed as a nucleotide sequence, then we can consider so called \\emph{secondary structures} of RNA~\\cite{RNA}, i.e., bonds between nucleotides of the RNA, with bonds being Watson-Crick pairs, i.e. hydrogen bonds between Adenine and Uracil and between Guanine and Cytosine, and stereo-chemical forces modelled by not allowing so called \\emph{pseudo-knots} (for details we refer to~\\cite{Ga1}). Then $\\ell(X)$ is the minimum number of non-bonded nucleotides for secondary structures of $X$. This is a biologically reasonable notion of energy.\n",
    "  \\item Again viewing $X=X^{(n)}$ as a word of length $n$ in the alphabet $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, we consider incomplete non-crossing matchings of the (indices of) letters in $X$ so that letters are matched with their inverses. Here a non-crossing matching is a set $P$ of pairs of indices $(i, j)$, $1\\leq i < j\\leq n$, such that\n",
    "        \\begin{enumerate}\n",
    "          \\item  each $i$ belongs to at most one element of $P$,\n",
    "          \\item  if $i<j<k<\\ell$, then at most one of $(i, k)$ and $(j,\\ell)$ belong to $P$,\n",
    "          \\item  if $(i, j)\\in P$ then $X_i = \\bar{X}_j$.\n",
    "        \\end{enumerate}\n",
    "        The length $\\ell(X)$ is the minimum number of unmatched letters over all non-crossing matchings. More generally we can take a random word in the alphabet with $2k$ letters $\\alpha_1$, $\\bar{\\alpha}_1$, \\dots, $\\alpha_k$, $\\bar{\\alpha}_k$ (where $\\bar{g}$ denotes $g^{-1})$ and consider non-crossing matchings with letters paired with their inverses, and define $\\ell$ as the minimum number of unmatched letters over all non-crossing matchings.\n",
    "\\end{enumerate}\n",
    "\n",
    "Henceforth, fix $k \\geq 2$ and consider a \\emph{random} string $X=X^{(n)}$ of length $n$ in $2k$ letters as above (i.e., a random word). The case $k = 2$ corresponds to RNA secondary structures, but most of our results and proofs are uniform in $k$. Let $L_{k}(n)=\\E\\l[\\ell(X^{(n)})\\r]$ where the expectation is over uniform distribution on strings of length $n$. Let $\\rho_{k}(n)=L_{k}(n)/n$ denote the average proportion of unpaired letters.\n",
    "\n",
    "Our main result is that this fraction converges to a positive constant.\n",
    "\\begin{theorem}\\label{main} With the above notations, $\\rho_{k}(n)\\rightarrow \\lam_{k}$ for some constant $0<\\lambda_k<1$.\n",
    "\\end{theorem}\n",
    "Thus, the average proportion of unpaired bases in an optimal secondary structure for a random RNA string converges to a \\emph{positive} constant as the length of the RNA string approaches infinity. Equivalently, for a word $X$ in the free group $\\F$ (or more generally in the free group $\\FF$ for $k\\geq 2$), the average ratio of the word length of $X$ in the (infinite) generating set consisting of conjugates of generators and their inverses to the word length of $X$ in the standard generators and their inverses converges to a positive constant. We remark that this result is also true, but essentially trivial, for the free group $\\mathbb{Z}$ on $1$ generator (for the group $\\mathbb{Z}$, the two generating sets, hence the corresponding word lengths, coincide).\n",
    "\n",
    "We also show  that  $\\ell(X^n)/n$ has exponential concentration in a window of length $1/\\sqrt{n}$ around its expectation $\\rho_k(n)$, and hence around $\\lambda_{k}$.\n",
    "\\begin{proposition}\\label{prop:concentration}\n",
    "  $\\P\\l\\{\\given \\ell(X^{(n)}) - n\\rho_k(n) \\given > t\\sqrt{n} \\r\\} \\le 2e^{-\\frac{t^{2}}{8}}$ for any $t>0$.\n",
    "\\end{proposition}\n",
    "An immediate corollary is that the standard deviation of $\\ell(X^n)$ is $O(\\sqrt{n})$.\n",
    "\n",
    "As for proofs, the existence of the limit $\\lambda_k$ and the exponential concentration are proved using sub-additivity and Hoeffding's inequality respectively, which are standard methods in combinatorial optimization problems. Showing that $\\lambda_k$ is strictly positive, and getting  bounds for its value require more involved arguments. It would be  interesting to find the exact value of $\\lambda_k$, particularly $\\lambda_2$. We are only able to get  bounds.\n",
    "\n",
    "For $k=2$, we prove the explicit bounds $0.034< \\lambda_2< 0.231$. The proof of Theorem~\\ref{main} given in Section~\\ref{S:Positive} gives the lower bound of $0.03$, which is then refined to get the slightly better lower bound of $0.034$. Elementary arguments in Section~\\ref*{S:upperbound} give an upper bound of $0.289$ which is improved to $\\frac{3}{13}=0.2307\\ldots$  in Section~\\ref{Sec:greedy}. This is achieved by analysing a specific algorithm for producing a non-crossing matching described below.\n",
    "\n",
    "\n",
    "\\subsection*{The one-sided greedy algorithm}   Scan the letters $X_{1},X_{2},\\ldots$  in that order and when the turn of $X_{t}$ comes (starting from $t=1$), match it to  $X_{s}$ with the largest value of $s<t$, if possible (i.e., $X_{s}=\\overline{X}_{t}$, and there is no $u\\in (s,t)$ such that $X_{u}=\\overline{X}_t$, and the non-crossing condition is maintained).\n",
    "\n",
    "For example, if $k=2$ and the word is $\\alpha \\beta\\alpha\\beta\\bar{\\alpha}\\alpha\\bar{\\beta}\\beta$, then the matching is $3\\leftrightarrow 5$, $2\\leftrightarrow 7$ (here $3,5,2,7$ represent the indices in the word, of course).\n",
    "\n",
    "\\begin{proposition}\\label{prop:onesidedgreedy} In the one-sided greedy algorithm, the proportion of unmatched letters converges to\n",
    "  \\begin{align}\\label{eq:boundforlambdakfromgreedy}\n",
    "    \\tilde{\\lambda}_k= 1-\\frac{\\sum_{r=1}^kr2^r\\binom{k}{r}\\prod_{j=1}^r\\frac{j(j+1)}{j(2k-j)-1}}{k\\sum_{r=0}^k\\binom{k}{r}2^r\\prod_{j=1}^r\\frac{j(j+1)}{j(2k-j)-1}}.\n",
    "  \\end{align}\n",
    "  Therefore $\\lambda_k\\le \\tilde{\\lambda}_k$.\n",
    "\\end{proposition}\n",
    "The numerical values of upper bound for the first few $k$ are\n",
    "\n",
    "\\begin{tabular}{c|c|c|c|c}\n",
    "  $k$                 & $2$                        & $3$                   & $4$                            & $5$                             \\\\ \\hline\n",
    "  $\\tilde{\\lambda}_k$ & $\\frac{3}{13}=0.231\\ldots$ & $\\frac{33}{100}=0.33$ & $\\frac{297}{455}=0.393\\ldots $ & $\\frac{3126}{7115}=0.439\\ldots$\n",
    "\\end{tabular}\n",
    "\n",
    "\\medskip\n",
    "Proposition~\\ref{prop:onesidedgreedy} is proved by analysing an associated  Markov chain on the space of words. This Markov chain is described in Section~\\ref{Sec:greedy}, where we also find its stationary distribution explicitly. It may be of  independent interest, as there are  not many examples of chains that are neither reversible nor have a doubly stochastic transition matrix for which we can solve for the stationary distribution exactly.\n",
    "\n",
    "There is some slack in our proofs, so our bounds can be sharpened. However our goal here is to give a simple and transparent proof.  In fact certain enumerative algorithms suggest that $\\lambda_2<0.11$ but we are unable to analyse these algorithms rigorously.\n",
    "\n",
    "\\subsection*{Dependence of $\\lambda_k$ on $k$} One may also ask about the behaviour of $\\lambda_k$ as a function of $k$. We claim that $\\lambda_k\\le \\lambda_{k+1}$. This is easiest seen by coupling. Consider a random word $X^n$ using symbols $\\alpha_i, \\bar{\\alpha}_i$, $1\\le i\\le k+1$. Let $X_{(j)}$ denote the word got by deleting all occurrences of $\\alpha_j,\\bar{\\alpha}_j$ in $X^{(n)}$, and let $N_j$ be the length of $X_{(j)}$. Let $\\ell_{(j)}$ denote the number of unmatched letters when  the optimal matching on $X^n$ is restricted to $X_{(j)}$. Then $\\ell_{(1)}+\\ldots +\\ell_{(k+1)}=k\\ell(X^n)$ and hence taking expectations and using symmetry, \n",
    "\\begin{align}\\label{eq:inequalityofLkLkplus1}\n",
    "(k+1)\\E[L_k(N_{1})]\\le k L_{k+1}(n).\n",
    "\\end{align}\n",
    "The expectation on the left is over the randomness in $N_{1}$ which has Binomial distribution with parameters $(n,k/(k+1))$. By Chebyshev's inequality, $\\P\\{n_-\\le N_1\\le n_+\\}\\ge 1-O(n^{-\\frac12})$, where $n_{\\pm}=\\frac{kn}{k+1}\\pm n^{\\frac34}$.  As $n\\mapsto L_k(n)$ is  obviously  increasing in $n$, \n",
    "\\[\n",
    "\\E[L_k(N_1)]\\ge (1-O(n^{-\\frac12}))L_k(n_-).\n",
    "\\]\n",
    "Combine this with \\eqref{eq:inequalityofLkLkplus1}, divide by $n$, and let $n\\to \\infty$ to get  $\\lambda_k\\le \\lambda_{k+1}$.\n",
    "\n",
    "Further, we show in Proposition~\\ref{prop:k-to-1} that $\\lambda_k\\to 1$ as $k\\to \\infty$.\n",
    "\n",
    "\\begin{remark}\n",
    "  As a consequence of the convergence of the fraction unmatched to a positive constant and the concentration result, it follows that there is some \\emph{scale} $N$ so that, for a generic RNA strand, optimal structures on pieces of length $N$ can be concatenated to give a near-optimal structure on the whole strand. As bonds at long distances are less likely to form, it follows that RNA folding can be localized to this scale, which makes foldings easier to analyse.\n",
    "\\end{remark}\n",
    "\n",
    "\\subsection*{Outline of the paper}  In Section~\\ref{sec:preliminaries} we show that the different ways of defining the length $\\ell$ outlined above give the same function. In Section~\\ref{S:Positive} we prove Theorem~\\ref{main} and the above-stated lower bounds for $\\lambda_2$. In Section~\\ref{S:upperbound} we present an elementary argument to obtain the upper bound of $0.289$ for $\\lambda_2$. In Section~\\ref{sec:concentration}, we prove Proposition~\\ref{prop:concentration}. In Section~\\ref{Sec:greedy} we introduce the Markov chain associated to the one-sided greedy algorithm, and  explicitly analyse it prove Proposition~\\ref{prop:onesidedgreedy}. In particular, this leads to the improved upper bound for $\\lambda_2$.\n",
    "\n",
    "\\section{Preliminaries}\\label{sec:preliminaries}\n",
    "\n",
    "For the convenience of the reader, we define length functions on groups and show that three definitions of the length $\\ell$ on $\\FF$ given above give the same function. The results in this section are elementary.\n",
    "\n",
    "\\begin{definition}\n",
    "  Let $G = (G, \\cdot, e, (\\cdot)^{-1})$ be a group (written multiplicatively,\n",
    "  with identity element $e$).  A \\emph{length function} on $G$ is a\n",
    "  map $l : G \\to [0,+\\infty)$ that obeys the properties\n",
    "  \\begin{itemize}\n",
    "    \\item $l(e) = 0$,\n",
    "    \\item $l(x)>0$, for all $x \\in G\\setminus \\{e\\}$,\n",
    "    \\item $l(x^{-1}) = l(x)$, for all $x,y\\in G$.\n",
    "    \\item $l (x y) \\leq l(x) + l(y)$, for all $x,y\\in G$.\n",
    "  \\end{itemize}\n",
    "\\end{definition}\n",
    "\n",
    "\\begin{definition}\n",
    "  We say that a length function $l$ is \\emph{conjugacy-invariant} if $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
    "\\end{definition}\n",
    "\n",
    "\n",
    "\n",
    "We shall see here that three definitions of a length $\\ell:\\FF \\to [0, \\infty)$ coincide. We also give more details of these definitions.\n",
    "\n",
    "\\subsection{Maximal length}\n",
    "Consider the set $\\mathcal{L}$ consisting of conjugacy-invariant length functions $l: \\FF\\to [0, +\\infty)$ satisfying $l(\\alpha_i)\\leq 1$ for all $1\\leq i\\leq k$. We have a partial order on length functions on $\\FF$ given by $l_1 \\leq l_2$ if and only if $l_1(g)\\leq l_2(g)$ for all $g\\in\\FF$. For this order, it is well known that there is a (necessarily unique, by properties of posets) maximal element. Namely, define\n",
    "$$\\ell_{max}(g) = \\sup\\{l(g): l\\in\\mathcal{L}\\}.$$\n",
    "\n",
    "Note that the set $\\{l(g): l\\in\\mathcal{L}\\}$ is bounded by the word length of $g$, so has a supremum.\n",
    "It is easy to see that $\\ell_{max}$ is a conjugacy-invariant length function, and that $\\ell(\\alpha_i)\\leq 1$ for all $1\\leq i\\leq k$. Thus $\\ell_{max}\\in \\mathcal{L}$. Further, by construction, if $l\\in\\mathcal{L}$, then $l\\leq \\ell_{max}$. Thus $\\ell_{max}$ is the maximum of the set $\\mathcal{L}$.\n",
    "\n",
    "\\subsection{Word length in conjugates of generators}\n",
    "\n",
    "Let $\\ell_{CW}: \\FF \\to [0, +\\infty)$ be the function given by the  word length in the generating set consisting of all conjugates of the generators $\\alpha_i$, $1\\leq i \\leq k$. Thus, for $g\\in\\FF$, $\\ell_{CW}(g)$ is the smallest value $r\\geq 0$ so that $g$ can be expressed as\n",
    "$$g = \\prod_{j=1}^r \\beta_j\\alpha_{i_j}^{\\epsilon_j}\\beta_j^{-1},$$\n",
    "where $\\beta_j\\in \\FF$ and $\\epsilon_j=\\pm 1$, for $1 \\leq j\\leq r$.\n",
    "\n",
    "\\begin{proposition}\n",
    "  We have $\\ell_{CW} = \\ell_{max}$.\n",
    "\\end{proposition}\n",
    "\\begin{proof}\n",
    "  We see that $\\ell_{CW}\\in\\mathcal{L}$. This is because the word length in a conjugacy-invariant set is a conjugacy-invariant length function, and $\\ell_{CW}(\\alpha_i) = 1$ for $1\\leq i\\leq k$.\n",
    "\n",
    "  Further, we see that $\\ell_{CW}$ is maximal. Namely, let $l\\in\\mathcal{L}$, $g\\in G$ and let $r = l_{CW}(g)$. Then we can express $g$ as $g = \\prod_{j=1}^r \\beta_j\\alpha_{i_j}^{\\epsilon_j}\\beta_j^{-1}$. By the triangle inequality, conjugacy-invariance, symmetry, and using $l(\\alpha_i)\\leq 1$ for $i\\leq i\\leq k$,\n",
    "  $$l(g)\\leq\\sum_{j=1}^r l(\\beta_j\\alpha_{i_j}^{\\epsilon_i}\\beta_j^{-1})\\leq\\sum_{j=1}^r l(\\alpha_{i_j}^{\\epsilon_i})\\leq \\sum_{j=1}^r 1 = r = \\ell_{CW}(g),$$\n",
    "  as required\n",
    "\n",
    "  As $\\ell_{CW}\\in \\mathcal{L}$ is maximal, $\\ell_{CW}=\\ell_{max}$.\n",
    "\\end{proof}\n",
    "\n",
    "\\subsection{Length from non-crossing matchings}\n",
    "\n",
    "Let $X^{(n)}=(X_{1},\\ldots ,X_{n})$ be a word in the alphabet with $2k$ letters $\\alpha_1$, $\\bar{\\alpha}_1$, \\dots, $\\alpha_k$, $\\bar{\\alpha}_k$. Let $NC$ stand for incomplete non-crossing matchings of $[n]= \\{1, 2,\\dots,n\\}$. Let $NC_{k}(X)$ be the subset of $M\\in NC$ such that for each matched pair $(i,j)\\in M$ we have $\\{X_{i},X_{j}\\}=\\{\\alp_{\\ell},\\bar{\\alp}_{\\ell}\\}$ for some $\\ell \\le k$.\n",
    "\n",
    "let $\\ell_{NC}(X)$ be the minimum number of unmatched pairs in all non-crossing matchings so that letters are paired with their inverses. We sketch the proofs that this is well-defined on $\\FF$, a conjugacy invariant length function and that $\\ell_{NC} = \\ell_{max}$. For more details, see~\\cite{Ga2} (which however has different terminology, and considers proofs for the case of two generators, though the proofs work just the same for general $k$).\n",
    "\n",
    "\\begin{lemma}\\label{welldefined}\n",
    "  Suppose $X_1$ and $X_2$ represent the same element in the group $\\FF$, then $\\ell_{NC}(X_1)= \\ell_{NC}(X_2)$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "  It suffices to consider the case where $X_1$ and $X_2$ are related by a single cancellation. Without loss of generality, assume that there exist words $W_1$ and $W_2$ and an index $1\\leq j\\leq k$ such that $X_1= W_1W_2$ and $X_2=W_1\\alpha_j\\alpha_j^{-1}W_2$. Let $\\mu_p$ be the length of $W_p$ for $p = 1,2$. Note that the cancelling pair corresponds to the pair $(\\mu_1+1, \\mu_1+ 2)$ of indices.\n",
    "\n",
    "  We show that $\\ell_{NC}(X_1)= \\ell_{NC}(X_2)$. First, fix a non-crossing matching $M_1$ of $X_1$ with $\\ell_{NC}(X_1)$ unmatched letters and with letters paired with their inverses. Let $\\sigma:\\N\\to\\N$ be defined by\n",
    "  $$\\sigma(m) = \\begin{cases}\n",
    "      m     & \\textrm{if $m\\leq \\mu_1$}, \\\\\n",
    "      m + 2 & \\textrm{if $m > \\mu_1$}\n",
    "    \\end{cases}$$\n",
    "  Then $M_2 := \\sigma(M_1)\\cup\\{(\\mu_1+1, \\mu_1 + 2)\\}\\in NC_k(X_2)$ and has $\\ell_{NC}(X_1)$ unmatched letters (i.e., the same as $M_1$). Hence $\\ell_{NC}(X_2)\\leq \\ell_{NC}(X_1)$.\n",
    "\n",
    "  Conversely, fix a non-crossing matching $M_2\\in NC_k(X_2)$ with $\\ell_{NC}(X_2)$ unmatched letters. Suppose at most one of $\\mu + 1$ and $\\mu + 2$ is matched in $M'$, and $(i, j)\\in M$ is the corresponding pair with $j\\in \\{\\mu +1, \\mu+ 2\\}$. Then $M_1 :=M_2\\setminus\\{(i, j)\\}\\in NC_k(X_1)$ and $M_1$ has at most $\\ell_{NC}(X_2)$ unmatched letters.\n",
    "\n",
    "  Next, if $(\\mu + 1, \\mu + 2)\\in M_2$, then $M_1:= M_2\\setminus\\{(\\mu + 1, \\mu + 2)\\}\\in NC_k(X_1)$ and $M_1$ has  $\\ell_{NC}(X_2)$ unmatched letters.\n",
    "\n",
    "  Finally, if for some indices $i$ and $j$ we have $(i, \\mu + 1)\\in M_2$ and $(j, \\mu + 2)\\in M_2$ (after possibly flipping some pairs), we can see that $$M_1:= M_2\\cup\\{(\\mu + 1, \\mu + 2)\\}\\setminus{\\{(i, \\mu + 1), (j, \\mu + 2)\\}}\\in NC_k(X_1)$$ and $M_1$ has $\\ell_{NC}(X_2)$ unmatched letters.\n",
    "\n",
    "  In all cases, we conclude that $\\ell_{NC}(X_1)\\leq \\ell_{NC}(X_2)$.\n",
    "\n",
    "\\end{proof}\n",
    "\n",
    "It follows that $\\ell_{NC}$ induces a well-defined function on $\\FF$, which we also denote as $\\ell_{NC}$. It is easy to see that it is a length function. The proof of the following is very similar to that of Lemma~\\ref{welldefined}.\n",
    "\n",
    "\\begin{lemma}\\label{conjinv}\n",
    "  Suppose $g, h\\in\\FF$, then $\\ell_{NC}(hgh^{-1})= \\ell_{NC}(g)$.\n",
    "\\end{lemma}\n",
    "\\qed\n",
    "\n",
    "It is easy to see that $\\ell_{NC}(\\alpha_i) = 1$ for all $1\\leq i\\leq k$, and that $\\ell_{NC}$ is symmetric. Thus $\\ell_{NC}\\in\\mathcal{L}$. Hence, to show that $\\ell_{NC} = \\ell_{max}$ it suffices to prove maximality, which we prove next.\n",
    "\n",
    "\\begin{lemma}\\label{maximal}\n",
    "  Suppose $l\\in\\mathcal{L}$ and $g\\in \\FF$. Then $l(g)\\leq\\ell_{NC}(g)$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "  Let $X$ be a word representing $g$. We prove the lemma by (strong) induction on the length $n$ of $X$. The case when the length is zero is clear.\n",
    "  Consider a non-crossing matching $M\\in NC_k(X)$ with $\\ell_{NC}(X)$ unmatched letters. First, suppose the index $1$ is unmatched in $M$, let $\\widehat{X}$ be obtained from $X$ by deleting the first letter. Then $M\\in NC_k(\\widehat{X})$, so by induction hypothesis, $l(\\widehat{X})\\leq \\ell_{NC}(\\widehat{X})$. Further, as $M$ restricted to $\\widehat{X}$ has one less unmatched letter than $M$, we conclude that $\\ell_{NC}(X) = \\ell_{NC}(\\widehat{X}) + 1$. As the first letter of $X$ is a generator or the inverse of a generator, using the triangle inequality\n",
    "  $$l(X)\\leq 1 + l(\\widehat{X})\\leq 1 + \\ell_{NC}(\\widehat{X})=\\ell_{NC}(X).$$\n",
    "\n",
    "  Next, if the pair $(1, j)\\in M$ with $j <n$, we split the word $X$ as $X = X_1 * X_2$ with $X_1$ of length $j$. Observe that the non-crossing condition implies that $M$ decomposes as $M_1\\cup M_2$ with $M_1\\in NC_k(X_1)$ and $M_2\\in NC_k(X_2)$. Again, we use the induction hypothesis and the triangle inequality to conclude that $l(X)\\leq \\ell_{NC}(X)$.\n",
    "\n",
    "  Finally, if $(1, n)\\in M$, let $\\widehat{X}$ be obtained from $X$ by deleting the first and last letter. By conjugacy invariance of $l$ and $\\ell_{NC}$, $l(X) = l(\\widehat{X})$ and $\\ell_{NC}(X)= \\ell_{NC}(\\widehat{X})$. Applying the induction hypothesis to $X$ gives the claim.\n",
    "\\end{proof}\n",
    "\n",
    "Thus, we can conclude the following.\n",
    "\n",
    "\\begin{proposition}\n",
    "  We have $\\ell_{NC} = \\ell_{max}$.\n",
    "\\end{proposition}\n",
    "\n",
    "\\section{The proportion of unmatched indices}\\label{S:Positive}\n",
    "In this section, we prove Theorem~\\ref{main} and get lower bounds on $\\lambda_{2}$. At first, $k$ is fixed, hence we drop it in the subscripts of $L(n)$ and $\\rho(n)$.\n",
    "\n",
    "The first observation is  that $L(n)$ is sub-additive.\n",
    "\n",
    "\\begin{lemma}\\label{lem:subadditivityofL}\n",
    "  For $m,n>0$, $L(m+n)\\leq L(m)+L(n)$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "  A string $X^{(m+n)}$ of i.i.d. random variables of length $m+n$  is obtained by taking the concatenation $X_1^{(n)}*X_2^{(m)}$ of two strings $X_1^{(n)}$ and $X_2^{(m)}$ of i.i.d. random variables of lengths $n$ and $m$ respectively. As the union of elements  $M_1=NC_k(X_1)$ and $M_2=NC_k(X_2)$ gives a matching $M\\in NC_k(X)$, it is easy to see that $\\ell(X)\\leq \\ell(X_1)+\\ell(X_2)$. By taking expectations the lemma follows.\n",
    "\\end{proof}\n",
    "\n",
    "As a well known consequence of sub-additivity (Fekete's lemma), we obtain the following.\n",
    "\n",
    "\\begin{corollary}\n",
    "  The sequence $\\rho(n)=L(n)/n$ converges to  $\\lambda_k:=\\inf_{n} \\rho (n)$.\n",
    "\\end{corollary}\n",
    "\n",
    "As $0\\leq \\rho(n)\\leq 1$, we get $0\\leq \\lambda_k\\leq 1$. It is easy to get some upper bounds for $\\lambda_k$ by computing $\\rho(n)$ for small $n$ (as $\\lambda_k$ is the infimum of $\\rho(n)$). For instance, for $k = 2$ and $n= 4$, $\\ell(X)$ takes values $0$, $2$ and $4$ with probabilities $28/256$, $168/256$ and $60/256$, respectively, hence $\\lambda_2\\leq \\rho(4) = 9/16$. The harder thing is to get lower bounds. Our main result is that $\\lambda_k$, which is the asymptotic proportion of unpaired bases, is positive.\n",
    "\n",
    "\\begin{lemma}\\label{lem:positivityofc0}\n",
    "  We have $\\lambda_k>0$.\n",
    "\\end{lemma}\n",
    "\\begin{proof}\n",
    "\n",
    "  Fix $\\delta >0$. Observe that $$L(n) \\geq n\\delta\\cdot \\P(\\ell(X^{(n)}) \\geq n\\delta),$$ and hence $$\\rho(n) \\geq \\delta \\P(\\ell(X^{(n)}) \\geq n\\delta).$$ Thus, if we have $\\P(\\ell(X^{(n)}) \\geq n\\delta) \\to 1$ as $n\\to\\infty$, then $\\lambda_k\\geq \\delta$. Thus it suffices to find a $\\delta>0$ for which we can show that $\\P(\\ell(X^{(n)}) \\geq n\\delta) \\to 1$,  or equivalently show that $$\\P(\\ell(X^{(n)}) <  n\\delta) \\to 0$$ as $n\\to\\infty$.\n",
    "\n",
    "  We shall now bound $\\P(\\ell(X^{(n)}) <  n\\delta)$ for small enough $\\delta$. Note that if $W(n, \\delta)$ is the number of words $X$ of length $n$ with $\\ell(X)< n\\delta$, then $$\\P(\\ell(X^{(n)}) <  n\\delta) = \\frac{W(n, \\delta)}{(2k)^n}.$$\n",
    "\n",
    "  Let $m=\\left\\lceil\\frac{n - n\\delta}{2}\\right\\rceil$ and let $r = n - 2m$. Observe that if $\\ell(X^{(n)})< n\\delta$, then $X= X^{(n)}$ has a non-crossing matching with at least $2m$ pairs, and hence a non-crossing matching $M$ with exactly $2m$ pairs (by simply dropping a few pairs). Given such an $M$, we can associate to $X$ a triple $(Y, Z, s)$ where\n",
    "  \\begin{itemize}\n",
    "    \\item $Y$ is the word (of length $r$) consisting of the letters of $X$ that are  \\emph{unmatched} in $M$, in the same order as in $X$,\n",
    "    \\item $Z$ is the word (of length $2m$) consisting of the letters of $X$ that are  \\emph{matched} in $M$, in the same order as in $X$, and,\n",
    "    \\item $s$ is the set of indices $i$, $1 \\leq i\\leq n$, that are \\emph{unmatched}.\n",
    "  \\end{itemize}\n",
    "\n",
    "  Note that $M$ gives a \\emph{complete non-crossing matching} on $Z$, and hence $Z$ represents the trivial word in $\\FF$. As the triple $(Y, Z, s)$ determines $X$, it follows that the number $W(n, \\delta)$ of words $X$ of length $n$ with $\\ell(X)< n\\delta$ is bounded above by the number of triples $(Y, Z, s)$, with\n",
    "  \\begin{itemize}\n",
    "    \\item $Y$ a word of length $r$,\n",
    "    \\item $Z$ a word of length $2m$ that represents the trivial element in the free group, and\n",
    "    \\item $s$ a subset of size $r$ of $\\{1, 2, \\dots,n \\}$.\n",
    "  \\end{itemize}\n",
    "\n",
    "  Let $T_p$ denote the set of words of length $p$ that represent the trivial element in the group $\\FF$. It follows that\n",
    "  \\begin{equation}\\label{triple-count}\n",
    "    |W(n, \\delta)| \\leq \\binom{n}{r}\\cdot (2k)^r\\cdot |T_{2m}|\n",
    "  \\end{equation}\n",
    "\n",
    "  The main step remaining is to bound $T_{2m}$. Let $\\tau_p = T_p/ (2k)^p$ represent the probability that a random word of length $p$ represents the trivial element in $\\FF$. We observe that this is the probability that the standard symmetric random walk on the Cayley graph of the free group (with the canonical generators and their inverses) starting at the identity returns to the identity in $p$ steps. It is clear that $\\tau_{p}\\tau_{q}\\le \\tau_{p+q}$, and hence by the Fekete lemma (applied to $\\log \\tau_{p}$), we see that $\\tau_{p}^{\\frac{1}{p}}\\to \\theta_{k}:=\\sup_{p}\\tau_{p}^{\\frac{1}{p}}$. This means that $\\tau_{p}\\le \\theta_{k}^{p}$ for each $p\\ge 1$. Of course $\\tau_{p}=0$ for odd $p$.\n",
    "\n",
    "  It is a known fact that $\\theta_{k}= \\sqrt{\\frac{2k-1}{k^2}}$ (for example Kesten~\\cite{Ke}). To see this, observe that the graph distance of the random walk to the identity element is itself a random walk on $\\N=\\{0,1,2,\\ldots\\}$ that goes from $i\\mapsto i+1$ with probability $(2k-1)/2k$ and $i\\mapsto i-1$ with probability $1/2k$, for $i\\ge 1$, and from $0$ to $1$ with probability $1$. The number of walks of length $p=2m$ that return to the origin in $\\N$ is the Catalan number $\\frac{1}{m+1}\\binom{2m}{m}$, and each such path (since it has $m$ up-steps and $m$ down-steps) has probability $(2k-1)^{m}/(2k)^{2m}$. Therefore,\n",
    "  \\begin{align*}\n",
    "    \\tau_{2m} & = \\frac{(2k-1)^{m}}{(2k)^{2m}(m+1)}\\binom{2m}{m}              \\\\\n",
    "              & \\sim \\frac{1}{\\sqrt{\\pi}m^{\\frac32}}\\frac{(2k-1)^{m}}{k^{2m}}\n",
    "  \\end{align*}\n",
    "  by Stirling's formula, where $a_{m}\\sim b_{m}$ means that $a_{m}/b_{m}$ converges to $1$ as $m\\to \\infty$. In particular, we see that $\\tau_{2m}^{\\frac{1}{2m}}\\to \\sqrt{\\frac{2k-1}{k^2}}$. Hence $\\theta_{k}=\\sqrt{\\frac{2k-1}{k^2}}$. In particular, $\\theta_2 = \\frac{\\sqrt{3}}{2}$, which we use for explicit estimates on $\\lambda_{2}$.\n",
    "\n",
    "  %\n",
    "  %\\begin{lemma}[Kesten~\\cite{Ke}]\\label{trivial-words}\n",
    "  %  Let $\\theta_k = \\sqrt{\\frac{2k-1}{k^2}}$. Then $\\tau_p\\leq \\theta_k^{p}$, hence $T_p\\leq\\theta_k^{p} (2k)^{p}$.\n",
    "  %\\end{lemma}\n",
    "  %\\begin{proof}\n",
    "  %  By Lemma~2.2 and Theorem~3 of~\\cite{Ke}, we see that $\\limsup_{p\\to\\infty} (\\tau_p)^{1/p} = \\theta_k$. Since the product of trivial words is trivial, for $n\\geq 1$ we have $\\tau_{kn} \\geq \\tau_p^n$, from which we deduce that $\\tau_p^{1/p}\\leq \\tau_{pn}^{1/pn}$. It follows that for all $p$, $\\tau_p^{1/p} \\leq \\limsup_{j\\to\\infty} (\\tau_{j})^{1/j} = \\theta_k$, i.e., $\\tau_p\\leq \\left(\\theta_k\\right)^{p}$. As there are $(2k)^p$ words of length $p$, $T_p \\leq \\left(\\theta_k\\right)^{p} (2k)^{p}$.\n",
    "  %\\end{proof}\n",
    "\n",
    "\n",
    "\n",
    "  It is now straightforward to complete the proof. For simplicity of notation, we ignore the error in rounding off to an integer and assume $r = n\\delta$. Using  the elementary fact that $\\binom{n}{r}\\le e^{nh(\\delta)}$ where $h(\\delta)= -\\delta\\log(\\delta) - (1 - \\delta)\\log(1-\\delta)$ in \\eqref{triple-count}, we get\n",
    "  $$\\P(\\ell(X^{(n)}) < n\\delta)\\leq \\exp\\left\\{n\\left(h(\\delta)+ \\log \\theta_k ) \\right) \\right\\}$$\n",
    "  Hence $\\P(\\ell(X^{(n)}) < n\\delta)\\to 0$ as $n\\to\\infty$ provided $h(\\delta)+ \\log\\theta_k < 0$.\n",
    "\n",
    "  When $k = 2$, as $\\theta_{2}=\\sqrt{3}/2$, this  happens, for example, for $\\delta = 0.03$. Thus, we have $\\lambda_2 > 0.03$, i.e. at least 3\\% of the letters are unmatched for the best non-crossing matching for most words.\n",
    "\n",
    "  Next, suppose $k\\to\\infty$. We see that $\\lambda_k\\to\\infty$.\n",
    "  \\begin{proposition}\\label{prop:k-to-1}\n",
    "    We have $\\lim_{k\\to\\infty} \\lambda_k = 1$.\n",
    "  \\end{proposition}\n",
    "  \\begin{proof}\n",
    "    Observe that $\\theta_{k}= \\sqrt{\\frac{2k-1}{k^2}}\\to 0$ as $k\\to\\infty$, hence $\\log(\\theta_k)\\to -\\infty$. It follows that for any fixed $\\delta\\in (0, 1)$,\n",
    "    if $k$ is sufficiently large we have $h(\\delta)+ \\log\\theta_k < 0$, hence $\\lambda_k > \\delta$. As\n",
    "    $\\lambda_k\\leq 1$ for all $k$, $\\lim_{k\\to\\infty} \\lambda_k = 1$.\n",
    "  \\end{proof}\n",
    "\n",
    "\n",
    "  Thus, we have shown that the limit $\\lambda_k$ of the sequence $\\rho(n)$ exists and is positive.\n",
    "  This completes the proof of Theorem~\\ref{main}, with the effective bound $\\lambda_2\\geq 0.03$ for $k = 2$ (other effective bounds can be computed similarly).\n",
    "\\end{proof}\n",
    "\n",
    "%To see that $\\lambda_k<1$, observe that on $S_n$, we have a matching with $\\tau$ bonds. \n",
    "%As, for $\\tau<\\frac{1}{64}$, the probability that a random string lies in $S_n$ tends to $1$, we can deduce that $\\lambda_k\\leq 1-\\frac{1}{64}$. \n",
    "\n",
    "\\subsection{Refinement of the lower bound for \\texorpdfstring{$\\lambda_{2}$}{L2} using maximal triples}\n",
    "\n",
    "We can refine the bound we obtained by choosing the triple $(Y, Z, s)$ in a canonical way (note that we do not, however, choose a canonical non-crossing matching on $Z$). Namely we try to match letters with as low indices as possible among all minimal non-crossing matchings. We fix $k = 2$ (so $\\FF = \\F$) in this subsection.\n",
    "\n",
    "First, observe that for fixed $X$, the words $Y$ and $Z$ are determined by $s$. More generally, given $X$, any subset $s\\subset [n]$ determines words $Y= Y(X, s)$ and $Z= Z(X, s)$, but in general the word $Z(X, s)$ may not represent the trivial element in $\\FF$. We shall say the triple $(Y(s), Z(s), s)$ determined by $s$ (and $X$) is \\emph{admissible} provided $Z$ represents the trivial element.\n",
    "\n",
    "The set $s$ can be viewed as a finite sequence by ordering its elements lexicographically, and two such sets can be compared using the lexicographic ordering on finite sequences, which is a total ordering. We order admissible triples $(Y, Z, s)$ by the component $s$ and choose the maximal admissible triple for each fixed $X$.\n",
    "\n",
    "We can decompose $s$ as $s = s_1\\cup s_2$, with $s_2$ (the {tail}) consisting of those elements $i\\in s$ such that if $j\\in [n]\\setminus s$, then $j < i$. Conversely, given an element $i \\in s_1$ there exists $j\\in [n]\\setminus s$ such that $j > i$. For $i\\in s_1$, let $\\hat{i}$ be the \\emph{smallest} element in $[n]\\setminus s$ such that $\\hat{i} > i$, i.e., $\\hat{i}$ is the first matched index after the unmatched index $i$. Geometrically, the unmatched indices $s$ are in general interspersed with the matched indices, with a (possibly empty) tail $s_2$ of unmatched indices which are larger than all matched indices.\n",
    "\n",
    "We claim that if $(X, Y, s)$ is maximal and $i\\in s_1$, then $X_i\\neq X_{\\hat{i}}$. For, if $X_i = X_{\\hat{i}}$, let $s' = s \\setminus\\{i\\}\\cup\\{\\hat{i}\\}$, $Y'= Y(X, s')$  and $Z' = Z(X, s')$. Then $Z' = Z$ as words in the free group, as the letter $X_{\\hat{i}}$ in $Z$ has been replaced by $X_i = X_{\\hat{i}}$ in $Z'$, and in the order on indices, $i$ has the same position in $Z'$ as $\\hat{i}$ has in $Z$ (this is because, if $j\\in [n]\\setminus(s\\cup s')$ is an index in both $Z$ and $Z'$, then $j\\leq i$ if and only if $j\\leq \\hat{i}$ by definition of $\\hat{i}$). Hence $Z'$ represents the trivial word. Hence the triple $(Y', Z', s')$ is admissible, and $s$ and $s'$ have the same cardinality. But $s < s'$, contradicting maximality of $s$.\n",
    "\n",
    "Thus, writing $Y = Y_1 * Y_2$ with $Y_i$ the word with letters $X_j$, $j\\in s_i$, and letting $r_i$ be the cardinality of $s_i$, we see that there are only $3^{r_1}4^{r_2}$ possibilities for the word $Y$ (corresponding to a maximal triple). On the other hand, the set $s_2$ is determined by $r_2$ as it consists of the last $r_2$ elements, and $s_1$ is a subset of size $r_1$ of the first $n - r_2 = n - r + r_1$ elements.\n",
    "\n",
    "Hence, using \\eqref{triple-count} once more and recalling that $\\theta_{2}=\\sqrt{3}/2$, we see that\n",
    "$$W(n, \\delta)\\leq \\left(\\sum\\limits_{r_1 = 0}^r \\binom{n - r + r_1}{r_1}3^{r_1}4^{r - r_1}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right)^{n-r} 4^{n - r}.$$\n",
    "\n",
    "We use $\\binom{n - r + r_1}{r_1}\\leq \\binom{n}{r_1}$ and the Chernoff bound for the tail of the binomial distribution to get\n",
    "\\begin{align*}\n",
    "  \\sum\\limits_{r_1 = 0}^r \\binom{n}{r_1}3^{r_1}4^{n - r_1} & \\leq 7^n\\exp\\left\\{-n\\left(\\delta\\log\\left(\\frac{\\delta}{3/7}\\right) +\n",
    "  (1 - \\delta)\\log\\left(\\frac{1-\\delta}{4/7} \\right)\\right) \\right\\}                                                                \\\\\n",
    "                                                           & = \\exp\\{n[h(\\delta)+\\delta \\log 3 + (1-\\delta)\\log 4]\\}.\n",
    "\\end{align*}\n",
    "Therefore, we get the improved bound\n",
    "\\begin{align*}\n",
    "  \\P(\\ell(X^{(n)}) < n\\delta) & \\leq |W(n,\\delta)|4^{-n}                                                 \\\\\n",
    "                              & \\le  \\exp\\{n[h(\\delta)+\\delta \\log(3/4)+(1-\\delta) \\log(\\sqrt{3}/2)] \\}.\n",
    "\\end{align*}\n",
    "%Using the above, we get the improved bound\n",
    "%\\begin{multline*}\n",
    "%  \\P(\\ell(X^{(n)}) < n\\delta)\\leq\\\\\n",
    "% \\exp\\left\\{n\\left(\\delta(-\\log(\\delta) + \\log(3/4)) + (1 - \\delta)(-\\log(1 - \\delta) + \\log(\\sqrt{3}/2)) \\right) \\right\\}\n",
    "%\\end{multline*} \n",
    "However the improved lower bound only gives a marginal improvement to $0.034$, i.e., at least 3.4\\% of the letters are unmatched on average.\n",
    "\n",
    "\\section{An elementary upper bound on \\texorpdfstring{$\\lambda_2$}{L2}}\\label{S:upperbound}\n",
    "We claim that $\\lambda_2\\le 0.29$ (we shall use more sophisticated methods to obtain a better bound in Section~\\ref{Sec:greedy}).  This is achieved as follows. Let $U_{1},V_{1},U_{2},V_{2},\\ldots $ be i.i.d. Geometric($1/2$) random variables, i.e.,  $\\P[U_{1}=j]=2^{-j}$ for $j\\ge 1$. Then $\\E[U_{1}]=2$, and hence with $m=\\lfloor n/4\\rfloor$ we get $N_{n}:=U_{1}+V_{1}+\\ldots +U_{m}+V_{m}=n+O(\\sqrt{n})$ with high probability. We create a string $S\\in \\{\\alpha,\\bar{\\alpha},\\beta,\\bar{\\beta}\\}^{N}$ by setting down a random string of $\\{\\alp,\\bar{\\alp}\\}$ of length $U_{1}$, then a random string of $\\{\\beta,\\bar{\\beta}\\}$ of length $V_{1}$, etc. Thus, $U_{i},V_{i}$ are the length of runs of the two species of symbols. This makes the length of the string random but since it is in a $\\sqrt{n}$ length window of $n$, this should not change anything much (as regards the proportion of unpaired sites). Consider the following matching algorithm.\n",
    "\n",
    "Fix any maximal noncrossing matching of all the $\\beta,\\bar{\\beta}$ symbols.  Then we make the best possible non-crossing matching of each run of $\\alpha,\\bar{\\alpha}$ within itself.  Thus, if the first run happens to be $\\alpha,\\bar{\\alpha},\\alpha$, then, we could match up the first two sites and leave the third one unpaired.\n",
    "\n",
    "In this matching scheme, in the first stage there are $O(\\sqrt{n})$ unpaired sites (the difference between the number of $\\beta$ and the number of $\\bar{\\beta}$ symbols in $S$). For the second stage, note that in the $j$th run (the one that has length $U_{j}$), the number of $\\alpha$-symbols is $\\xi_{j}\\sim \\mb{Binomial}(U_{j},\\frac{1}{2})$, and hence the number of left overs is $|2\\xi_{j}-U_{j}|$. The total number of left over sites has expectation $m\\E[|2\\xi_{1}-U_{1}|] + O(\\sqrt{n})$ which gives us the bound\n",
    "\\begin{align*}\n",
    "  \\lambda_2\\le \\frac{1}{4}\\E[|2\\xi_{1}-U_{1}|].\n",
    "\\end{align*}\n",
    "Numerical evaluation of the expectation (expressed as an infinite sum) gives the bound $\\lambda_2\\le 0.2886...$.\n",
    "\n",
    "\n",
    "\n",
    "\\section{Concentration around expected behaviour}\\label{sec:concentration}\n",
    "We prove Proposition~\\ref{prop:concentration} in this section.\n",
    "%Lemma~\\ref{lem:positivityofc0} shows that  $L(X^{(n)})/n$ gets close to $\\lambda_k$ on average, as $n\\to \\infty$. For real applications one would like to say that for most strings of length $n$, the random quantity $L(X^{(n)})/n$ is itself close to $\\lambda_{k}$, along with a quantitative estimate on how close it is. Using standard tools of probability we now show that $L(X^{(n)})/n$ is concentrated around its expectation in a window of size $1/\\sqrt{n}$.\n",
    "%\\begin{lemma}\\label{lem:concentration} With the notations of the previous section\n",
    "%  $$\n",
    "%    \\P\\l[\\given L(X^{(n)}) - \\E[L(X^{(n)})] \\given > t\\sqrt{n} \\r] \\le 2e^{-\\frac{t^{2}}{8}}, \\qquad \\mb{for any }t>0.\n",
    "%  $$\n",
    "%\\end{lemma}\n",
    "The tool is the well-known Hoeffding's inequality for sums of martingale differences (see section 4.1 of Ledoux~\\cite{ledoux} for a proof and the book of Steele~\\cite{steele} for its use in many combinatorial optimization problems similar to ours). It says that if  $d_{1},d_{2},\\ldots ,d_{n}$ is a martingale difference sequence, that is $\\E[d_{j}\\vert d_{1},\\ldots d_{j-1}]=0$ for each $j$ (for $j=1$ this is to be interpreted as $\\E[d_{1}]=0$) and $|d_{j}|\\le B_{j}$ with probability $1$ for some constant $B_{j}$, then for any $t>0$, we have\n",
    "\\begin{equation} \\label{eq:hoeffding}\n",
    "  \\P\\l[\\big| \\sum_{j=1}^{n}d_{j} \\big| >t \\r]\\le 2e^{-\\frac{t^{2}}{2(B_{1}^{2}+\\ldots +B_{n}^{2})}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{proof}[Proof of Proposition~\\ref{prop:concentration}] Let $X=X^{(n)}=(X_{1},\\ldots ,X_{n})$. Define for $j=1,\\ldots n$,\n",
    "  $$\n",
    "    d_{j} = \\E\\l[L(X_{1},\\ldots ,X_{n}) \\given X_{1},\\ldots ,X_{j} \\r] - \\E\\l[L(X_{1},\\ldots ,X_{n}) \\given X_{1},\\ldots ,X_{j-1} \\r].\n",
    "  $$\n",
    "  Then $d_{j}$ is a martingale difference sequence by the tower property\n",
    "  \\begin{align*}\n",
    "    \\E[\\E[U \\given V,W]\\given W]=\\E[U\\given W].\n",
    "  \\end{align*}\n",
    "  Further, $L(X)-\\E[L(X)] = d_{1}+\\ldots +d_{n}$. If we show that $|d_{j}|\\le 2$, then by applying Hoeffding's inequality~\\eqref{eq:hoeffding}, we get the statement in the lemma.\n",
    "\n",
    "  To prove that $|d_{j}|\\le 2$, fix $j$ and let $Y=(X_{1},\\ldots ,X_{j-1},X_{j}',X_{j+1},\\ldots ,X_{n})$ where $X_{j}'$ is an independent copy of $X_{j}$ that is also independent of all $X_{j}$s. Then,\n",
    "  $$\n",
    "    \\E[L(Y)\\given X_{1},\\ldots ,X_{j}] \\; =\\;  \\E[L(Y)\\given X_{1},\\ldots ,X_{j-1}]  \\; =  \\; \\E[L(X)\\given X_{1},\\ldots ,X_{j-1}],\n",
    "  $$\n",
    "  where the first equality holds because $X_{j}$ is independent of $Y$ and the second equality holds because $X_{1},\\ldots X_{j-1}$ bear the same relationship to $X$ as to $Y$. Thus, we conclude that\n",
    "  $$\n",
    "    d_{j} = \\E\\l[L(X)-L(Y) \\given X_{1},\\ldots X_{j} \\r].\n",
    "  $$\n",
    "  But $X$ and $Y$ differ only in one co-ordinate. From any non-crossing matching of $X$, by deleting the edge (if any) matching the $j$th co-ordinate, we obtain a non-crossing matching for $Y$ with at most two more unmatched indices. Therefore $L(Y)\\le L(X)+2$ and by symmetry between $X$ and $Y$, we get $|L(X)-L(Y)|\\le 2$. Therefore $|d_{j}| \\le \\E\\l[|L(X)-L(Y)| \\given X_{1},\\ldots X_{j} \\r] \\le 2$.\n",
    "\\end{proof}\n",
    "\n",
    "\n",
    "\n",
    "\\section{Greedy algorithms and Upper bounds}\\label{Sec:greedy}\n",
    "%Any algorithm for generating a non-crossing matching leads to an upper bound for $\\lambda_k$.  Here we consider one particular one-sided greedy algorithm that can be rigorously analysed by framing it in terms of a Markov chain, and get the bound $\\lambda_{2}\\le 3/13$. Simulations give much better bounds of $0.11$ from enumeration based algorithms, {\\color{red} but they are more difficult to analyse}. \n",
    "%\n",
    "%\\subsection{The one-sided greedy algorithm}   Scan the letters $X_{1},X_{2},\\ldots$  in that order and when the turn of $X_{t}$ comes (starting from $t=1$), match it to  $X_{s}$ with the largest value of $s<t$, if possible (i.e., $X_{s}=\\overline{X}_{t}$, and there is no $u\\in (s,t)$ such that $X_{u}=\\overline{X}_t$, and the non-crossing condition is maintained). For example, if $k=2$ and the word is $1212\\bar{1}1\\bar{2}2$, then the matching is $3\\leftrightarrow 5$, $2\\leftrightarrow 7$. \n",
    "\n",
    "The goal of this section is to prove Proposition~\\ref{prop:onesidedgreedy}. First we introduce a Markov chain related to this algorithm. Recall the description of the algorithm from the introduction.\n",
    "\n",
    "\\subsection{The associated Markov chain and its stationary distribution} For simplicity of notation, we write the alphabet set as $\\mathbb A=\\{1,\\bar{1},\\ldots ,k,\\bar{k}\\}$. Let $w[t]$ be the word formed by all the {\\em accessible} letters at ``time'' $t$ -- these are the letters among $X_1,\\ldots ,X_t$ that are   still available for matching in future in the above greedy algorithm. Then $w[t]$ is a Markov chain whose state space is  $\\Omega=\\mathbb A^0\\sqcup \\mathbb A^1\\sqcup \\mathbb A^2\\sqcup \\ldots$, the set of all finite strings in the  alphabet $\\mathbb A$ (including the empty string) and whose dynamics  are as follows:\n",
    "\n",
    "If $w[t]=(w_{1},\\ldots ,w_{p})$ and $X_{t+1}=x$, then $w[t+1]=(w_1,\\ldots ,w_p,x)$ if $\\bar{x}$ does not occur in $w[t]$. Otherwise  $w[t+1]=(w_{1},\\ldots ,w_{j-1})$ where $j$ is the largest index such that $w_j=\\bar{x}$. Two letters get matched each time the length of $w[t]$ reduces. Hence the number left unmatched after $n$ steps is $n-2\\sum_{t=2}^n\\mathbf 1_{\\mb{\\tiny length}(w[t])<\\mb{\\tiny length}(w[t-1])}$.\n",
    "\n",
    "The Markov chain  is not irreducible. From any state it is possible to go to $\\emptyset$ but from $\\emptyset$ the chain can only go to states in\n",
    "\\[\n",
    "  \\Omega_0=\\{w\\in \\Omega \\; : \\; \\mb{ at most one of }x,\\bar{x}\\in w\\mb{ for each }x\\in \\mathbb A\\}\n",
    "\\]\n",
    "which makes $\\Omega_0$ the unique irreducible class. As we shall show next, this Markov chain has a stationary probability distribution $\\pi$. By the general theory of Markov chains, the stationary distribution is unique. To give the formula for $\\pi$, we need some notation.\n",
    "\n",
    "For a word $w\\in \\Omega_0$, define $a_i(w)$ inductively by declaring $a_1(w)+\\ldots +a_j(w)$ to be the length of the maximal initial segment in $w$ (reading from the left) containing at most $j$ distinct symbols. Note that if $w$ has only $j$ different symbols from $\\mathbb A$, it follows that $a_i(w)=0$ for $i\\ge j$. In particular, as $w\\in \\Omega_0$, it has at most $k$ distinct symbols.  For example, if $k=3$ and $w=11212212311232$, then $(a_1,a_2,a_3)=(2,8,6)$. If $w=22212$ then $(a_1,a_2,a_3)=(3,2,0)$. For the empty word, $a_i(w)=0$ for all $i$.\n",
    "\n",
    "\\begin{proposition}\\label{prop:statdist} Fix $k\\ge 2$. Let $\\tau_j=\\frac{j+1}{j(2k+1)-1}$ for $1\\le j\\le k$. Then the unique stationary probability distribution is given by\n",
    "  \\begin{align*}\n",
    "    \\pi(w)=\\frac{1}{\\mb{Z}}\\tau_1^{a_1(w)}\\tau_2^{a_2(w)}\\ldots \\tau_k^{a_k(w)}\n",
    "  \\end{align*}\n",
    "  where $Z=\\sum_{r=0}^k\\binom{k}{r}2^r\\prod_{j=1}^r\\frac{j\\tau_j}{1-j\\tau_j}=\\sum_{r=0}^k\\binom{k}{r}2^r\\prod_{j=1}^r\\frac{j(j+1)}{j(2k-j)-1}$.\n",
    "\\end{proposition}\n",
    "Assuming this proposition, we prove Proposition~\\ref{prop:onesidedgreedy}.\n",
    "% from $\\lambda_k$ that we get from \\eqref{eq:boundforlambdakintermsofstatdist}. \n",
    "\n",
    "\\subsection{Proof of Proposition~\\ref{prop:onesidedgreedy}}  From the earlier observation, the expected proportion of matched letters after $n$ steps is\n",
    "\\begin{align*}\n",
    "  \\frac{2}{n}\\sum_{k=1}^n\\P\\{\\mb{length}(w[t])<\\mb{length}(w[t-1])\\} \\to 2\\P_{\\pi}\\{\\mb{length}(w[1])<\\mb{length}(w[0])\\}\n",
    "\\end{align*}\n",
    "where the subscript $\\pi$ is to indicate that $w[0]$ is sampled from $\\pi$ (in the actual chain, we start with $w[0]=\\emptyset$) and the convergence follows from the general theory of Markov chains which asserts that the distribution of $(w[t-1],w[t])$ (from any starting point) converges to the distribution of $(w[0],w[1])$ when $w[0]$ has distribution $\\pi$. As a consequence, we arrive at the upper bound\n",
    "\\begin{align}\\label{eq:boundforlambdakintermsofstatdist}\n",
    "  \\lambda_k\\le 1-2\\P_{\\pi}\\{\\mb{length}(w[1])<\\mb{length}(w[0])\\}.\n",
    "\\end{align}\n",
    "If $w$ has $r$ distinct symbols, then $a_r(w)>0 (=a_{r+1}(w))$ and its length gets reduced if and only if the next arriving letter can match up with one of them, i.e., with probability $\\frac{r}{2k}$.  Further, for a given choice of strictly positive integers $a_1,\\ldots ,a_r$, the number of words $w$ with $a_i(w)=a_i$ is precisely\n",
    "\\begin{align}\\label{eq:numberofwforgivenai}\n",
    "  2^rk(k-1)\\ldots (k-r+1)2^{a_2-1}3^{a_3-1}\\ldots r^{a_r-1}.\n",
    "\\end{align}\n",
    "Here $2k-2i+2$ is for the choice of $i$th new symbol (the locations are determined by $a_1,\\ldots ,a_r$) and the $a_j-1$ letters between the $j$th new symbol and $(j+1)$st new symbol each have $j$ choices, hence the factor of $j^{a_j-1}$.\n",
    "Thus,\n",
    "\\begin{align*}\n",
    "  \\P_{\\pi}\\{\\mb{length}(w[1])<\\mb{length}(w[0])\\} & =\\frac{1}{2k Z}\\sum_{r=1}^kr2^r\\binom{k}{r}\\sum_{a_i\\ge 1: i\\le r}\\prod_{j=1}^r(j \\tau_j)^{a_j} \\\\\n",
    "                                                  & =\\frac{1}{2k Z}\\sum_{r=1}^kr2^r\\binom{k}{r}\\prod_{j=1}^r\\frac{j\\tau_j}{1-j\\tau_j}.\n",
    "  %&=\\frac{1}{2k Z}\\sum_{r=1}^kr2^r\\binom{k}{r}\\prod_{j=1}^r\\frac{j(j+1)}{j(2k-j)-1}.\n",
    "\\end{align*}\n",
    "Substituting the value of $\\tau_j$ given in the statement of the proposition,  \\begin{align}\\label{eq:boundforlambdak}\n",
    "  \\tilde{\\lambda}_k= 1-\\frac{1}{kZ}\\sum_{r=1}^kr2^r\\binom{k}{r}\\prod_{j=1}^r\\frac{j(j+1)}{j(2k-j)-1}.\n",
    "\\end{align}\n",
    "Plugging in the expression for $Z$ given in Proposition~\\ref{prop:statdist} completes the proof of  Proposition~\\ref{prop:onesidedgreedy}. \\hfill $\\qed$\n",
    "\n",
    "\\noindent{\\bf Case $k=2$:} This is the case we care most about. We see that $\\tau_1=\\frac12$ and $\\tau_2=\\frac13$ and $Z=13$. Hence $\\pi(w)=\\frac{1}{13}\\l(\\frac32\\r)^{a_1(w)}\\l(\\frac13\\r)^{\\mb{ \\tiny length}(w)}$ where $a_1(w)$ is the length of the first run (i.e., the maximum $j$ such that $w_1=w_2=\\ldots =w_j$). Therefore, \\eqref{eq:boundforlambdak} becomes\n",
    "\\[\n",
    "  \\tilde{\\lambda}_2= 1-\\frac{1}{2\\times 13}\\l( 4+16\\r)=\\frac{3}{13}=0.2307\\ldots\n",
    "\\]\n",
    "\n",
    "\\subsection{Proof of Proposition~\\ref{prop:statdist}} Let $\\sigma(w)=\\tau_1^{a_1(w)}\\ldots \\tau_k^{a_k(w)}$. If $w$ has $r$ distinct symbols, then $a_j\\ge 1$ for $j\\le r$ and $a_j=0$ for $j>r$. The number of words $w$ with given $a_1,\\ldots ,a_r$ is given in \\eqref{eq:numberofwforgivenai}. Hence the sum of $\\sigma(w)$ over such $w$ is\n",
    "\\[\n",
    "  2^r\\binom{k}{r}\\sum_{a_1,\\ldots ,a_r\\ge 1}\\prod_{j=1}^r(j\\tau_j)^{a_j}=2^r\\binom{k}{r}\\prod_{j=1}^r\\frac{j\\tau_j}{1-j\\tau_j}.\n",
    "\\]\n",
    "Sum over $r$ (including $r=0$) to get the given expression for $Z$.\n",
    "\n",
    "It suffices to check that $\\sigma$ satisfies the equations for the stationary distribution, since we know the uniqueness (up to scalar multiples) of stationary distribution. The general equations are\n",
    "\\[\n",
    "  \\sum_{w:w'\\mapsto w}\\sigma(w')=2k\\sigma(w)\n",
    "\\]\n",
    "where the notation $w'\\mapsto w$ means that $w'$ can lead to $w$ in one step (in our Markov chain, a given $w'$ can lead to a given $w$ in at most one way, hence the transition probability is exactly $1/2k$). If $w=(w_1,\\ldots ,w_p)$ has  exactly $r$ distinct symbols, then $a_r(w)>0=a_{r+1}(w)$, and   $\\sigma(w)=\\tau_1^{a_1(w)}\\ldots \\tau_r^{a_r(w)}$. The possible $w'$ are:\n",
    "\\begin{enumerate}\n",
    "  \\item $w'=(w_1,\\ldots ,w_{p-1})$. Then $a_i(w')=a_i(w)$ for $i\\le r-1$ and $a_r(w')=a_r(w)-1$.\n",
    "  \\item $w'=wxy^1t_1y^2\\ldots t_jy^{j+1}$ where $x\\in \\mathbb A$ is a symbol that occurs in $w$ and $t_i\\in \\mathbb A$ are the new symbols that did not occur before and $y^i=(y_{1}^i,\\ldots,y^i_{m_i})$ with $m_i\\ge 0$ . Here $j$ can vary from $0$ to $k-r$. Further, $x$ should not occur in $y^1t_1y^2\\ldots t_jy^{j+1}$ so that $w'$ can lead to $w$ when an $\\bar{x}$ arrives (it is tacit that all our words are in $\\Omega_0$, so we do not write  those conditions again). Then\n",
    "        \\[\n",
    "          a_{i}(w')=\\begin{cases} a_i(w) & \\mb{ if }i\\le r-1, \\\\ a_r(w)+m_1+1 &\\mb{ if }i=r, \\\\ m_{i-r+1}+1 &\\mb{ if }r+1\\le i\\le r+j. \\end{cases}\n",
    "        \\]\n",
    "        For given $j$ and $m_1,\\ldots ,m_j$, the number of choices of such $w'$ is\n",
    "        \\[\n",
    "          2^j(k-r)(k-r-1)\\ldots (k-r-j+1) \\times r (r-1)^{m_1}r^{m_2}\\ldots (r+j-1)^{m_{j+1}}.\n",
    "        \\]\n",
    "        This is because there are $2k-2r-2i+2$ choices for $t_i$ and $r+i-2$ choices for each letter in $y^i$.\n",
    "\n",
    "  \\item $w'=wt_1y^1t_2y^2\\ldots t_jy^{j}$ where  $y^i=(y_{1}^i,\\ldots,y^i_{m_i})$ with $m_i\\ge 0$ and $t_i\\in \\mathbb A$ are the new symbols that did not occur before. Here $j$ can vary from $1$ to $k-r$. Further, $t_1$ should not occur in $y^1t_2y^2\\ldots t_jy^{j}$. Then\n",
    "        \\[\n",
    "          a_{i}(w')=\\begin{cases} a_i(w) & \\mb{ if }i\\le r,  \\\\ m_{i-r}+1 &\\mb{ if }r+1\\le i\\le r+j. \\end{cases}\n",
    "        \\]\n",
    "        For given $j$ and $m_1,\\ldots ,m_j$, the number of choices of such $w'$ is\n",
    "        \\[\n",
    "          2^j(k-r)(k-r-1)\\ldots (k-r-j+1) \\times r^{m_1}(r+1)^{m_2}\\ldots (r+j-1)^{m_{j}}.\n",
    "        \\]\n",
    "        Here $2k-2r-2i+2$ is the number of choices for $t_i$ and $r+i-1$ is the number of choices for each letter in $y^i$.\n",
    "\\end{enumerate}\n",
    "Using these and cancelling common factors, the equation for stationary distribution becomes\n",
    "\\begin{align*}\n",
    "  2k\\tau_r & =1+\\frac{r\\tau_r^2}{1-(r-1)\\tau_r}\\sum_{j=0}^{k-r} 2^j(k-r)_{j\\downarrow} \\prod_{i=r+1}^{r+j}\\frac{\\tau_i}{1-(i-1)\\tau_i}                                              \\\\\n",
    "           & \\;\\;\\;\\;\\;\\;\\;+\\tau_r\\sum_{j=1}^{k-r} 2^j(k-r)_{j\\downarrow} \\prod_{i=r+1}^{r+j}\\frac{1}{1-(i-1)\\tau_i}                                                                \\\\\n",
    "           & =1+\\frac{r\\tau_r^2}{1-(r-1)\\tau_r}+\\l(\\frac{r\\tau_r^2}{1-(r-1)\\tau_r}+\\tau_r\\r)\\sum_{j=1}^{k-r} 2^j(k-r)_{j\\downarrow} \\prod_{i=r+1}^{r+j}\\frac{\\tau_i}{1-(i-1)\\tau_i}\n",
    "\\end{align*}\n",
    "which is the same as (empty products are interpreted as $1$)\n",
    "\\[\n",
    "  (2k+1)\\tau_r-1=\\frac{\\tau_r(\\tau_r+1)}{1-(r-1)\\tau_r}\\sum_{j=0}^{k-r}  \\prod_{i=r+1}^{r+j}\\frac{2(k+1-i)\\tau_i}{1-(i-1)\\tau_i}, \\;\\;\\; 0\\le r\\le k.\n",
    "\\]\n",
    "Notice that the sum on the right is of the form\n",
    "$1+u_{r+1}+u_{r+1}u_{r+2}+\\ldots = 1+u_{r+1}\\l( 1+u_{r+2}+u_{r+2}u_{r+3}+\\ldots \\r)$, and the quantity in brackets on the right occurs in exactly that form in the equation for $r+1$. Therefore, the above equation can be re-written for $r<k$ as\n",
    "\\[\n",
    "  \\frac{((2k+1)\\tau_r-1)(1-(r-1)\\tau_r)}{\\tau_r(\\tau_r+1)}=1+\\frac{2(k-r)\\tau_{r+1}}{1-r\\tau_{r+1}}\\frac{((2k+1)\\tau_{r+1}-1)(1-r\\tau_{r+1})}{\\tau_{r+1}(\\tau_{r+1}+1)}.\n",
    "\\]\n",
    "Plugging in the stated values of $\\tau_r$ and $\\tau_{r+1}$, a short calculation shows that both sides are equal to $(2k+2-r)/r$, hence equality holds.\n",
    "\n",
    "For $r=k$, the original equation is $(2k+1)\\tau_k-1=\\frac{\\tau_k(\\tau_k+1)}{1-(k-1)\\tau_k}$ which is easily seen to be satisfied by $\\tau_k=1/(2k-1)$.  This completes the proof. \\hfill \\qed\n",
    "\n",
    "\\begin{remark} Although the proof is more or less straightforward checking with some calculations, it hinged on having the form of the stationary distribution. All features of the stationary distribution, namely the product form with exponents being $a_i$s and the values of $\\tau_i$s were arrived at by extensive checking on Mathematica software for several values of $k$, along with some guess work. On a computer, one must restrict to finite state space chains, and a natural restriction is to words of length at most $L$ (steps outside this are forbidden). If $\\pi_L$ is the stationary distribution of this Markov chain, then not only does $\\pi_L$ converge to $\\pi$, but curiously $\\pi_L(w)=\\pi(w)$ for all $w$ of length $L-1$ or less!\n",
    "\n",
    "\\end{remark}\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d22a6b2a-dca1-4ee7-aa04-01c07186583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a research paper on sequences of letters or nucleotides, related to RNA sequences and free groups. The authors define a length function $\\ell$ on a sequence $X$ of letters (nucleotides or generators of a free group), and show that it has several equivalent definitions. For instance, when $X$ is a word in a free group $\\F$, $\\ell$ can be defined as the maximal conjugacy-invariant length function, satisfying certain conditions. Alternatively, if $X$ is an RNA sequence, $\\ell(X)$ can be defined as the minimum number of non-bonded nucleotides for secondary structures of $X$. \n",
      "\n",
      "The authors' main result is a theorem stating that for a random string of length $n$ in $2k$ letters, the average proportion of unpaired letters in an optimal secondary structure converges to a positive constant. This implies that the average ratio of the word length of $X$ in the generating set consisting of conjugates of generators and their inverses to the word length of $X$ in the standard generators and their inverses converges to a positive constant. The authors prove this result using methods from combinatorial optimization, and also show that the standard deviation of $\\ell(X^n)$ is $O(\\sqrt{n})$. \n",
      "\n",
      "The authors then discuss bounds for the constant $\\lambda_k$, which is the limit of the average proportion of unpaired letters. They obtain explicit bounds for the case $k = 2$, and show that $\\lambda_k$ increases with $k$ and approaches $1$ as $k$ tends to infinity. They also analyze a greedy algorithm for matching letters, and show that the proportion of unmatched letters in this algorithm provides an upper bound for $\\lambda_k$. Lastly, they define a Markov chain related to the greedy algorithm, and use it to calculate the stationary distribution and improve their upper bound for $\\lambda_2$.\n",
      "\n",
      "---------------------------\n",
      "The mathematical text discusses a length function, denoted as $\\ell$, for RNA sequences or words in a free group. The function $\\ell$ is defined in three ways: 1) As the maximal conjugacy-invariant length function on the free group satisfying certain constraints. 2) In the context of an RNA sequence, $\\ell$ is the minimum number of non-bonded nucleotides for secondary structures of the sequence. 3) For words of length $n$, $\\ell$ is the minimum number of unmatched letters over all non-crossing matchings.\n",
      "\n",
      "The text also discusses a random string of length $n$ and the expected value of the length function, denoted as $L_k(n)$. Another important quantity is $\\rho_k(n)$, the average proportion of unpaired letters in a string of length $n$.\n",
      "\n",
      "The main result is that the fraction $\\rho_k(n)$ converges to a positive constant denoted as $\\lambda_k$ for some $0<\\lambda_k<1$ as $n$ approaches infinity. This is stated as theorem 1 in the text. The text also presents proposition 1 which shows that $\\ell(X^n)/n$ has exponential concentration in a window of length $1/\\sqrt{n}$ around its expectation $\\rho_k(n)$, and hence around $\\lambda_{k}$.\n",
      "\n",
      "The text then provides a detailed explanation of how to prove these statements and provides bounds for $\\lambda_k$, particularly for $k=2$. It introduces a one-sided greedy algorithm for generating non-crossing matchings and uses it to prove proposition 2, which provides an upper bound for $\\lambda_k$.\n",
      "\n",
      "For large $k$, it's shown that $\\lambda_k$ converges to 1. The text also discusses a Markov chain associated to the one-sided greedy algorithm, finds its stationary distribution, and uses it to calculate the proportion of unmatched letters. The paper outlines the proof for the existence of $\\lambda_k$ and the exponential concentration using sub-additivity and Hoeffding's inequality respectively.\n",
      "---------------------------\n",
      "This mathematical text focuses on the topic of non-crossing matchings of letters in words or sequences, specifically in the context of RNA sequences and the free group. It discusses three definitions of length for a word or sequence, and proves that these three definitions are in fact equivalent.\n",
      "\n",
      "The first definition of length is based on free group theory, where the length of a word is defined as the maximal conjugacy-invariant length function on the free group that satisfies certain conditions. This length definition can be generalized to an arbitrary word in 2k letters, where k is an integer greater than or equal to 2.\n",
      "\n",
      "The second definition of length is related to biology, where the length of an RNA sequence is defined as the minimum number of non-bonded nucleotides among all possible secondary structures of the sequence. Here, bonds represent Watson-Crick pairs and stereo-chemical forces are modeled by not allowing pseudo-knots.\n",
      "\n",
      "The third definition of length, applicable to a word of length n in an alphabet with 2k letters, is defined using the concept of non-crossing matchings, where letters are matched with their inverses. The length is the minimum number of unmatched letters over all non-crossing matchings.\n",
      "\n",
      "The text then introduces the concept of a random string of length n in 2k letters. The main theorem of the text states that the average proportion of unpaired letters in a random string converges to a positive constant as the length of the string approaches infinity. This statement is applicable to both RNA sequences and words in the free group. \n",
      "\n",
      "The text also presents a proposition that the standard deviation of the number of unmatched letters in a word is O(sqrt(n)). The proofs of the main theorem and this proposition are based on combinatorial optimization methods such as sub-additivity and Hoeffding's inequality.\n",
      "\n",
      "The text concludes by considering the dependence of the limit constant on the number of letters k in the alphabet. It is shown that the limit constant increases with k and tends to 1 as k approaches infinity. The text suggests that this result may be useful in studying the folding of RNA strands. Finally, the text mentions a certain algorithm for producing a non-crossing matching, and estimates the upper bound for the limit constant in the case of two-letter alphabets.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "ncs = summarise(non_cross)\n",
    "\n",
    "print_all(ncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b1c6fc2-14b4-41e3-86d9-266f0fbcc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definitions present in the provided text are:\n",
      "\n",
      "1. A word $X$ is defined as a string of characters chosen from a specific alphabet. The alphabet can have $4$ letters and can be viewed either as $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$ (generators of the free group $\\F$ and their inverses, where $\\bar{g}$ stands for $g^{-1}$), or as $A$, $U$, $G$ and $C$ (nucleotides in an RNA sequence).\n",
      "\n",
      "2. The length $\\ell(X)$ of the word $X$ is a function that can be defined in several ways.\n",
      "\n",
      "3. One way to define $\\ell$ is the maximal conjugacy-invariant length function on $\\F$, which satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$. \n",
      "\n",
      "4. Equivalently, $\\ell$ can be defined as the word length in the generating set given by all conjugates $g\\alpha g^{-1}$, $g\\bar{\\alpha} g^{-1}$, $g\\bar{\\beta} g^{-1}$ and $g\\beta g^{-1}$ of the generators of $\\F$ and their inverses.\n",
      "\n",
      "5. Another way to define $\\ell$ is the minimum number of non-bonded nucleotides for secondary structures of $X$ in the context of a nucleotide sequence.\n",
      "\n",
      "6. In the case of viewing $X$ as a word of length $n$ in the alphabet $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, $\\ell$ can be defined as the minimum number of unmatched letters over all non-crossing matchings.\n",
      "\n",
      "7. A non-crossing matching is defined as a set $P$ of pairs of indices $(i, j)$, $1\\leq i < j\\leq n$, with the properties that each $i$ belongs to at most one element of $P$, if $i<j<k<\\ell$, then at most one of $(i, k)$ and $(j,\\ell)$ belong to $P$, and if $(i, j)\\in P$ then $X_i = \\bar{X}_j$.\n",
      "\n",
      "8. A random string $X^{(n)}$ of length $n$ in $2k$ letters is defined as a word chosen with uniform probability from all possible words of length $n$ in the $2k$ letter alphabet.\n",
      "\n",
      "9. $L_{k}(n)=\\E\\l[\\ell(X^{(n)})\\r]$ is defined as the expected value of the length function $\\ell$ when $X^{(n)}$ is a random string of length $n$ in $2k$ letters.\n",
      "\n",
      "10. $\\rho_{k}(n)=L_{k}(n)/n$ is defined as the average proportion of unpaired letters in a random string of length $n$ in $2k$ letters.\n",
      "---------------------------\n",
      "In the presented text, the following definitions were provided:\n",
      "\n",
      "1. A word $X$ is defined as an element from an alphabet consisting of $4$ letters, such as $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, or $A$, $U$, $G$ and $C$. \n",
      "\n",
      "2. The length $\\ell(X)$ of a word $X$ is a function that can be defined in a few equivalent ways.\n",
      "\n",
      "3. The maximal conjugacy-invariant length function on a free group $\\F$ is defined as a length function $\\ell$ that satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$.\n",
      "\n",
      "4. A secondary structure of RNA is defined as a particular arrangement of nucleotides in an RNA sequence that satisfies certain bonding rules.\n",
      "\n",
      "5. A non-crossing matching is defined as a set $P$ of pairs of indices $(i, j)$, $1\\leq i < j\\leq n$, such that each index $i$ belongs to at most one pair, if $i<j<k<\\ell$, then at most one of $(i, k)$ and $(j,\\ell)$ belong to $P$, and if $(i, j)\\in P$ then $X_i = \\bar{X}_j$.\n",
      "\n",
      "6. A random string $X=X^{(n)}$ of length $n$ in $2k$ letters is defined as a word in an alphabet with $2k$ letters.\n",
      "\n",
      "7. The average proportion of unpaired letters $\\rho_{k}(n)=L_{k}(n)/n$ is defined as the ratio of the expectation of the number of unpaired letters $L_{k}(n)$ to the total length of the string $n$.\n",
      "\n",
      "8. A length function on a group $G$ is a map $l : G \\to [0,+\\infty)$ that satisfies several properties.\n",
      "\n",
      "9. A conjugacy-invariant length function $l$ on a group $G$ is a length function that satisfies $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
      "\n",
      "---------------------------\n",
      "The definitions stated in the provided text are as follows:\n",
      "\n",
      "1. A word $X$ is considered in an alphabet consisting of $4$ letters, with the letters viewed either as $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$ or $A$, $U$, $G$ and $C$.\n",
      "\n",
      "2. The length of such a word $\\ell(X)$, which is a function associated to the word, is defined in three different ways: \n",
      "\n",
      "    a. If $X$ is viewed as a word in $\\F$, then $\\ell$ is the maximal conjugacy-invariant length function on $\\F$ which satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$.\n",
      "\n",
      "    b. If $X$ is viewed as a nucleotide sequence, $\\ell(X)$ is defined as the minimum number of non-bonded nucleotides for secondary structures of $X$.\n",
      "\n",
      "    c. Viewing $X=X^{(n)}$ as a word of length $n$ in the alphabet $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, we consider incomplete non-crossing matchings of the letters in $X$ and define $\\ell$ as the minimum number of unmatched letters over all non-crossing matchings.\n",
      "\n",
      "3. A random string $X=X^{(n)}$ of length $n$ in $2k$ letters where $L_{k}(n)=\\E[\\ell(X^{(n)})]$ is the expectation over uniform distribution on strings of length $n$. \n",
      "\n",
      "4. The average proportion of unpaired letters $\\rho_{k}(n)=L_{k}(n)/n$ is defined.\n",
      "\n",
      "5. A \\emph{length function} on a group $G$ is a map $l : G \\to [0,+\\infty)$ that obeys certain properties.\n",
      "\n",
      "6. A \\emph{conjugacy-invariant} length function $l$ obeys the property that $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
      "\n",
      "In the subsection named \"Maximal length,\" $\\ell_{max}$ is defined as the supremum of all conjugacy-invariant length functions on $\\FF$ that satisfy certain conditions.\n",
      "\n",
      "In the subsection \"Word length in conjugates of generators,\" $\\ell_{CW}: \\FF \\to [0, +\\infty)$ is defined as the function given by the word length in the generating set consisting of all conjugates of the generators.\n",
      "\n",
      "In the subsection \"Length from non-crossing matchings,\" $\\ell_{NC}$ is defined as the minimum number of unmatched pairs in all non-crossing matchings of a certain type.\n",
      "\n",
      "In the subsection \"The proportion of unmatched indices,\" $\\lambda_k$ is defined as the limit of the sequence $\\rho(n)=L(n)/n$ as $n$ tends to infinity.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "def_query = f'''State precisely all the definitions (and only the definitions) in the following text:\n",
    "\n",
    "{non_cross}\n",
    "'''\n",
    "\n",
    "nc_dfs = math(def_query, deployment_name='leanaide-gpt4-32')\n",
    "\n",
    "print_all(nc_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6758403-c9fd-45d3-8ae3-2c10902f6f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `definition` environments in the provided text are:\n",
      "\n",
      "\\begin{definition}\n",
      "  A \\emph{length function} on a group $G = (G, \\cdot, e, (\\cdot)^{-1})$ (written multiplicatively, with identity element $e$) is a map $l : G \\to [0,+\\infty)$ that obeys the properties\n",
      "  \\begin{itemize}\n",
      "    \\item $l(e) = 0$,\n",
      "    \\item $l(x)>0$, for all $x \\in G\\setminus \\{e\\}$,\n",
      "    \\item $l(x^{-1}) = l(x)$, for all $x,y\\in G$.\n",
      "    \\item $l (x y) \\leq l(x) + l(y)$, for all $x,y\\in G$.\n",
      "  \\end{itemize}\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "  A length function $l$ is \\emph{conjugacy-invariant} if $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
      "\\end{definition}\n",
      "\n",
      "It should be noted that while these are the definitions explicitly stated in the given text, a number of additional concepts and structures are implicitly defined within the text. For example, the text implicitly defines the notions of an alphabet, a word in an alphabet, the length of a word, the notion of a sequence of RNA, and the concept of a non-crossing matching of the (indices of) letters in a word.\n",
      "---------------------------\n",
      "In the given text, the following definitions are explicitly stated:\n",
      "\n",
      "1. A word $X$ is considered in an alphabet consisting of $4$ letters. These letters are viewed as $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$ (i.e., generators of the free group $\\F$ and their inverses). Alternatively, the letters are also viewed as $A$, $U$, $G$ and $C$ (i.e., nucleotides in an RNA sequence).\n",
      "\n",
      "2. The length $\\ell(X)$ of a word $X$ is described in three ways:\n",
      "\n",
      "   (a) If $X$ is viewed as a word in $\\F$, then $\\ell$ is the maximal conjugacy-invariant length function on $\\F$ which satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$.\n",
      "\n",
      "   (b) If $X$ is viewed as a nucleotide sequence, then $\\ell(X)$ is the minimum number of non-bonded nucleotides for secondary structures of $X$.\n",
      "\n",
      "   (c) If $X=X^{(n)}$ is viewed as a word of length $n$ in the alphabet $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, then $\\ell(X)$ is the minimum number of unmatched letters over all non-crossing matchings of the (indices of) letters in $X$.\n",
      "\n",
      "3. A random string $X=X^{(n)}$ of length $n$ in $2k$ letters is considered. The average proportion of unpaired letters in $X$ is denoted by $\\rho_{k}(n)=L_{k}(n)/n$ where $L_{k}(n)=\\E\\l[\\ell(X^{(n)})\\r]$ is the expected length of $X$.\n",
      "\n",
      "4. A \\emph{length function} on a group $G = (G, \\cdot, e, (\\cdot)^{-1})$ is a map $l : G \\to [0,+\\infty)$ that obeys certain properties.\n",
      "\n",
      "5. A \\emph{conjugacy-invariant} length function $l$ on a group $G$ is one that satisfies $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
      "\n",
      "The \\LaTeX{} code for these definitions is as follows:\n",
      "\n",
      "\\begin{verbatim}\n",
      "\\begin{definition}\n",
      "A \\emph{word} $X$ is considered in an alphabet consisting of $4$ letters. These letters are viewed as either $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$ (i.e., generators of the free group $\\F$ and their inverses) or as $A$, $U$, $G$ and $C$ (i.e., nucleotides in an RNA sequence).\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "The \\emph{length} $\\ell(X)$ of a word $X$ is defined in three equivalent ways:\n",
      "\n",
      "(a) If $X$ is viewed as a word in $\\F$, then $\\ell$ is the maximal conjugacy-invariant length function on $\\F$ which satisfies $\\ell(\\alpha)\\leq 1$ and $\\ell(\\beta)\\leq 1$.\n",
      "\n",
      "(b) If $X$ is viewed as a nucleotide sequence, then $\\ell(X)$ is the minimum number of non-bonded nucleotides for secondary structures of $X$.\n",
      "\n",
      "(c) If $X=X^{(n)}$ is viewed as a word of length $n$ in the alphabet $\\alpha$, $\\bar{\\alpha}$, $\\beta$ and $\\bar{\\beta}$, then $\\ell(X)$ is the minimum number of unmatched letters over all non-crossing matchings of the (indices of) letters in $X$.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "For a \\emph{random} string $X=X^{(n)}$ of length $n$ in $2k$ letters, let $L_{k}(n)=\\E\\l[\\ell(X^{(n)})\\r]$ denote the expected length of $X$. The average proportion of unpaired letters in $X$ is denoted by $\\rho_{k}(n)=L_{k}(n)/n$.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "Let $G = (G, \\cdot, e, (\\cdot)^{-1})$ be a group. A \\emph{length function} on $G$ is a map $l : G \\to [0,+\\infty)$ that satisfies the following properties:\n",
      "\\begin{itemize}\n",
      "  \\item $l(e) = 0$,\n",
      "  \\item $l(x)>0$, for all $x \\in G\\setminus \\{e\\}$,\n",
      "  \\item $l(x^{-1}) = l(x)$, for all $x,y\\in G$,\n",
      "  \\item $l (x y) \\leq l(x) + l(y)$, for all $x,y\\in G$.\n",
      "\\end{itemize}\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "A length function $l$ on a group $G$ is \\emph{conjugacy-invariant} if $l(xyx^{-1}) = l(y)$ for all $x,y\\in G$.\n",
      "\\end{definition}\n",
      "\\end{verbatim}\n",
      "---------------------------\n",
      "There are no definitions in the given text.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "def_query = f'''State precisely all the definitions (and only the definitions) in the following text, with each definition in LaTeX format as a `definition` environment:\n",
    "\n",
    "{non_cross}\n",
    "'''\n",
    "\n",
    "nc_dfs = math(def_query, deployment_name='leanaide-gpt4-32')\n",
    "\n",
    "print_all(nc_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432ced3-5d75-4e40-a2d5-a0934c803fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
